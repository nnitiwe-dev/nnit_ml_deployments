[
{"title": "Transformers in Healthcare: A Survey", "authors": ["Subhash Nerella", "Sabyasachi Bandyopadhyay", "Jiaqing Zhang", "Miguel Contreras", "Scott Siegel", "Aysegul Bumin", "Brandon Silva", "Jessica Sena", "Benjamin Shickel", "Azra Bihorac", "Kia Khezeli", "Parisa Rashidi"], "url": "https://arxiv.org/abs/2307.00067", "abstract": "With Artificial Intelligence (AI) increasingly permeating various aspects of\nsociety, including healthcare, the adoption of the Transformers neural network\narchitecture is rapidly changing many applications. Transformer is a type of\ndeep learning architecture initially developed to solve general-purpose Natural\nLanguage Processing (NLP) tasks and has subsequently been adapted in many\nfields, including healthcare. In this survey paper, we provide an overview of\nhow this architecture has been adopted to analyze various forms of data,\nincluding medical imaging, structured and unstructured Electronic Health\nRecords (EHR), social media, physiological signals, and biomolecular sequences.\nThose models could help in clinical diagnosis, report generation, data\nreconstruction, and drug/protein synthesis. We identified relevant studies\nusing the Preferred Reporting Items for Systematic Reviews and Meta-Analyses\n(PRISMA) guidelines. We also discuss the benefits and limitations of using\ntransformers in healthcare and examine issues such as computational cost, model\ninterpretability, fairness, alignment with human values, ethical implications,\nand environmental impact."},
{"title": "Qualitative Prediction of Multi-Agent Spatial Interactions", "authors": ["Sariah Mghames", "Luca Castri", "Marc Hanheide", "Nicola Bellotto"], "url": "https://arxiv.org/abs/2307.00065", "abstract": "Deploying service robots in our daily life, whether in restaurants,\nwarehouses or hospitals, calls for the need to reason on the interactions\nhappening in dense and dynamic scenes. In this paper, we present and benchmark\nthree new approaches to model and predict multi-agent interactions in dense\nscenes, including the use of an intuitive qualitative representation. The\nproposed solutions take into account static and dynamic context to predict\nindividual interactions. They exploit an input- and a temporal-attention\nmechanism, and are tested on medium and long-term time horizons. The first two\napproaches integrate different relations from the so-called Qualitative\nTrajectory Calculus (QTC) within a state-of-the-art deep neural network to\ncreate a symbol-driven neural architecture for predicting spatial interactions.\nThe third approach implements a purely data-driven network for motion\nprediction, the output of which is post-processed to predict QTC spatial\ninteractions. Experimental results on a popular robot dataset of challenging\ncrowded scenarios show that the purely data-driven prediction approach\ngenerally outperforms the other two. The three approaches were further\nevaluated on a different but related human scenarios to assess their\ngeneralisation capability."},
{"title": "Novelty and Lifted Helpful Actions in Generalized Planning", "authors": ["Chao Lei", "Nir Lipovetzky", "Krista A. Ehinger"], "url": "https://arxiv.org/abs/2307.00735", "abstract": "It has been shown recently that successful techniques in classical planning,\nsuch as goal-oriented heuristics and landmarks, can improve the ability to\ncompute planning programs for generalized planning (GP) problems. In this work,\nwe introduce the notion of action novelty rank, which computes novelty with\nrespect to a planning program, and propose novelty-based generalized planning\nsolvers, which prune a newly generated planning program if its most frequent\naction repetition is greater than a given bound $v$, implemented by\nnovelty-based best-first search BFS($v$) and its progressive variant PGP($v$).\nBesides, we introduce lifted helpful actions in GP derived from action schemes,\nand propose new evaluation functions and structural program restrictions to\nscale up the search. Our experiments show that the new algorithms BFS($v$) and\nPGP($v$) outperform the state-of-the-art in GP over the standard generalized\nplanning benchmarks. Practical findings on the above-mentioned methods in\ngeneralized planning are briefly discussed."},
{"title": "Solving Multi-Agent Target Assignment and Path Finding with a Single  Constraint Tree", "authors": ["Yimin Tang", "Zhongqiang Ren", "Jiaoyang Li", "Katia Sycara"], "url": "https://arxiv.org/abs/2307.00663", "abstract": "Combined Target-Assignment and Path-Finding problem (TAPF) requires\nsimultaneously assigning targets to agents and planning collision-free paths\nfor agents from their start locations to their assigned targets. As a leading\napproach to address TAPF, Conflict-Based Search with Target Assignment (CBS-TA)\nleverages both K-best target assignments to create multiple search trees and\nConflict-Based Search (CBS) to resolve collisions in each search tree. While\nbeing able to find an optimal solution, CBS-TA suffers from scalability due to\nthe duplicated collision resolution in multiple trees and the expensive\ncomputation of K-best assignments. We therefore develop Incremental Target\nAssignment CBS (ITA-CBS) to bypass these two computational bottlenecks. ITA-CBS\ngenerates only a single search tree and avoids computing K-best assignments by\nincrementally computing new 1-best assignments during the search. We show that,\nin theory, ITA-CBS is guaranteed to find an optimal solution and, in practice,\nis computationally efficient."},
{"title": "Neuro-Symbolic Sudoku Solver", "authors": ["Ashutosh Hathidara", "Lalit Pandey"], "url": "https://arxiv.org/abs/2307.00653", "abstract": "Deep Neural Networks have achieved great success in some of the complex tasks\nthat humans can do with ease. These include image recognition/classification,\nnatural language processing, game playing etc. However, modern Neural Networks\nfail or perform poorly when trained on tasks that can be solved easily using\nbacktracking and traditional algorithms. Therefore, we use the architecture of\nthe Neuro Logic Machine (NLM) and extend its functionality to solve a 9X9 game\nof Sudoku. To expand the application of NLMs, we generate a random grid of\ncells from a dataset of solved games and assign up to 10 new empty cells. The\ngoal of the game is then to find a target value ranging from 1 to 9 and fill in\nthe remaining empty cells while maintaining a valid configuration. In our\nstudy, we showcase an NLM which is capable of obtaining 100% accuracy for\nsolving a Sudoku with empty cells ranging from 3 to 10. The purpose of this\nstudy is to demonstrate that NLMs can also be used for solving complex problems\nand games like Sudoku. We also analyze the behaviour of NLMs with a\nbacktracking algorithm by comparing the convergence time using a graph plot on\nthe same problem. With this study we show that Neural Logic Machines can be\ntrained on the tasks that traditional Deep Learning architectures fail using\nReinforcement Learning. We also aim to propose the importance of symbolic\nlearning in explaining the systematicity in the hybrid model of NLMs."},
{"title": "Inertial Navigation Meets Deep Learning: A Survey of Current Trends and  Future Directions", "authors": ["Nadav Cohen", "Itzik Klein"], "url": "https://arxiv.org/abs/2307.00014", "abstract": "Inertial sensing is used in many applications and platforms, ranging from\nday-to-day devices such as smartphones to very complex ones such as autonomous\nvehicles. In recent years, the development of machine learning and deep\nlearning techniques has increased significantly in the field of inertial\nsensing. This is due to the development of efficient computing hardware and the\naccessibility of publicly available sensor data. These data-driven approaches\nare used to empower model-based navigation and sensor fusion algorithms. This\npaper provides an in-depth review of those deep learning methods. We examine\nseparately, each vehicle operation domain including land, air, and sea. Each\ndomain is divided into pure inertial advances and improvements based on filter\nparameters learning. In addition, we review deep learning approaches for\ncalibrating and denoising inertial sensors. Throughout the paper, we discuss\nthese trends and future directions. We also provide statistics on the commonly\nused approaches to illustrate their efficiency and stimulate further research\nin deep learning embedded in inertial navigation and fusion."},
{"title": "Minimum Levels of Interpretability for Artificial Moral Agents", "authors": ["Avish Vijayaraghavan", "Cosmin Badea"], "url": "https://arxiv.org/abs/2307.00660", "abstract": "As artificial intelligence (AI) models continue to scale up, they are\nbecoming more capable and integrated into various forms of decision-making\nsystems. For models involved in moral decision-making, also known as artificial\nmoral agents (AMA), interpretability provides a way to trust and understand the\nagent's internal reasoning mechanisms for effective use and error correction.\nIn this paper, we provide an overview of this rapidly-evolving sub-field of AI\ninterpretability, introduce the concept of the Minimum Level of\nInterpretability (MLI) and recommend an MLI for various types of agents, to aid\ntheir safe deployment in real-world settings."},
{"title": "Black-Box Prediction of Flaky Test Fix Categories Using Language Models", "authors": ["Sakina Fatima", "Hadi Hemmati", "Lionel Briand"], "url": "https://arxiv.org/abs/2307.00012", "abstract": "Flaky tests are problematic because they non-deterministically pass or fail\nfor the same software version under test, causing confusion and wasting\ndeveloper time. While machine learning models have been used to predict\nflakiness and its root causes, there is less work on providing support to fix\nthe problem. To address this gap, we propose a framework that automatically\ngenerates labeled datasets for 13 fix categories and train models to predict\nthe fix category of a flaky test by analyzing the test code only. Though it is\nunrealistic at this stage to accurately predict the fix itself, the categories\nprovide precise guidance about what part of the test code to look at. Our\napproach is based on language models, namely CodeBERT and UniXcoder, whose\noutput is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese\nNetwork-based Few Shot Learning (FSL). Our experimental results show that\nUniXcoder outperforms CodeBERT, in correctly predicting most of the categories\nof fixes a developer should apply. Furthermore, FSL does not appear to have any\nsignificant effect. Given the high accuracy obtained for most fix categories,\nour proposed framework has the potential to help developers to fix flaky tests\nquickly and <a class=\"link-external link-http\" href=\"http://accurately.To\" rel=\"external noopener nofollow\">this http URL</a> aid future research, we make our automated labeling\ntool, dataset, prediction models, and experimental infrastructure publicly\navailable."},
{"title": "The Integer Linear Programming Inference Cookbook", "authors": ["Vivek Srikumar", "Dan Roth"], "url": "https://arxiv.org/abs/2307.00171", "abstract": "Over the years, integer linear programs have been employed to model inference\nin many natural language processing problems. This survey is meant to guide the\nreader through the process of framing a new inference problem as an instance of\nan integer linear program and is structured as a collection of recipes. At the\nend, we will see two worked examples to illustrate the use of these recipes."},
{"title": "Investigating Masking-based Data Generation in Language Models", "authors": ["Ed S. Ma"], "url": "https://arxiv.org/abs/2307.00008", "abstract": "The current era of natural language processing (NLP) has been defined by the\nprominence of pre-trained language models since the advent of BERT. A feature\nof BERT and models with similar architecture is the objective of masked\nlanguage modeling, in which part of the input is intentionally masked and the\nmodel is trained to predict this piece of masked information. Data augmentation\nis a data-driven technique widely used in machine learning, including research\nareas like computer vision and natural language processing, to improve model\nperformance by artificially augmenting the training data set by designated\ntechniques. Masked language models (MLM), an essential training feature of\nBERT, have introduced a novel approach to perform effective pre-training on\nTransformer based models in natural language processing tasks. Recent studies\nhave utilized masked language model to generate artificially augmented data for\nNLP downstream tasks. The experimental results show that Mask based data\naugmentation method provides a simple but efficient approach to improve the\nmodel performance. In this paper, we explore and discuss the broader\nutilization of these data augmentation methods based on MLM."},
{"title": "ChatGPT vs. Google: A Comparative Study of Search Performance and User  Experience", "authors": ["Ruiyun Xu", "Yue Feng", "Hailiang Chen"], "url": "https://arxiv.org/abs/2307.01135", "abstract": "The advent of ChatGPT, a large language model-powered chatbot, has prompted\nquestions about its potential implications for traditional search engines. In\nthis study, we investigate the differences in user behavior when employing\nsearch engines and chatbot tools for information-seeking tasks. We carry out a\nrandomized online experiment, dividing participants into two groups: one using\na ChatGPT-like tool and the other using a Google Search-like tool. Our findings\nreveal that the ChatGPT group consistently spends less time on all tasks, with\nno significant difference in overall task performance between the groups.\nNotably, ChatGPT levels user search performance across different education\nlevels and excels in answering straightforward questions and providing general\nsolutions but falls short in fact-checking tasks. Users perceive ChatGPT's\nresponses as having higher information quality compared to Google Search,\ndespite displaying a similar level of trust in both tools. Furthermore,\nparticipants using ChatGPT report significantly better user experiences in\nterms of usefulness, enjoyment, and satisfaction, while perceived ease of use\nremains comparable between the two tools. However, ChatGPT may also lead to\noverreliance and generate or replicate misinformation, yielding inconsistent\nresults. Our study offers valuable insights for search engine management and\nhighlights opportunities for integrating chatbot technologies into search\nengine designs."},
{"title": "Towards Explainable AI for Channel Estimation in Wireless Communications", "authors": ["Abdul Karim Gizzini", "Yahia Medjahdi", "Ali J. Ghandour", "Laurent Clavier"], "url": "https://arxiv.org/abs/2307.00952", "abstract": "Research into 6G networks has been initiated to support a variety of critical\nartificial intelligence (AI) assisted applications such as autonomous driving.\nIn such applications, AI-based decisions should be performed in a real-time\nmanner. These decisions include resource allocation, localization, channel\nestimation, etc. Considering the black-box nature of existing AI-based models,\nit is highly challenging to understand and trust the decision-making behavior\nof such models. Therefore, explaining the logic behind those models through\nexplainable AI (XAI) techniques is essential for their employment in critical\napplications. This manuscript proposes a novel XAI-based channel estimation\n(XAI-CHEST) scheme that provides detailed reasonable interpretability of the\ndeep learning (DL) models that are employed in doubly-selective channel\nestimation. The aim of the proposed XAI-CHEST scheme is to identify the\nrelevant model inputs by inducing high noise on the irrelevant ones. As a\nresult, the behavior of the studied DL-based channel estimators can be further\nanalyzed and evaluated based on the generated interpretations. Simulation\nresults show that the proposed XAI-CHEST scheme provides valid interpretations\nof the DL-based channel estimators for different scenarios."},
{"title": "DisCo: Disentangled Control for Referring Human Dance Generation in Real  World", "authors": ["Tan Wang", "Linjie Li", "Kevin Lin", "Chung-Ching Lin", "Zhengyuan Yang", "Hanwang Zhang", "Zicheng Liu", "Lijuan Wang"], "url": "https://arxiv.org/abs/2307.00040", "abstract": "Generative AI has made significant strides in computer vision, particularly\nin image/video synthesis conditioned on text descriptions. Despite the\nadvancements, it remains challenging especially in the generation of\nhuman-centric content such as dance synthesis. Existing dance synthesis methods\nstruggle with the gap between synthesized content and real-world dance\nscenarios. In this paper, we define a new problem setting: Referring Human\nDance Generation, which focuses on real-world dance scenarios with three\nimportant properties: (i) Faithfulness: the synthesis should retain the\nappearance of both human subject foreground and background from the reference\nimage, and precisely follow the target pose; (ii) Generalizability: the model\nshould generalize to unseen human subjects, backgrounds, and poses; (iii)\nCompositionality: it should allow for composition of seen/unseen subjects,\nbackgrounds, and poses from different sources. To address these challenges, we\nintroduce a novel approach, DISCO, which includes a novel model architecture\nwith disentangled control to improve the faithfulness and compositionality of\ndance synthesis, and an effective human attribute pre-training for better\ngeneralizability to unseen humans. Extensive qualitative and quantitative\nresults demonstrate that DISCO can generate high-quality human dance images and\nvideos with diverse appearances and flexible motions. Code, demo, video and\nvisualization are available at: <a class=\"link-external link-https\" href=\"https://disco-dance.github.io/\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "Towards Brain Inspired Design for Addressing the Shortcomings of ANNs", "authors": ["Fahad Sarfraz", "Elahe Arani", "Bahram Zonooz"], "url": "https://arxiv.org/abs/2307.00039", "abstract": "As our understanding of the mechanisms of brain function is enhanced, the\nvalue of insights gained from neuroscience to the development of AI algorithms\ndeserves further consideration. Here, we draw parallels with an existing\ntree-based ANN architecture and a recent neuroscience study[27] arguing that\nthe error-based organization of neurons in the cerebellum that share a\npreference for a personalized view of the entire error space, may account for\nseveral desirable features of behavior and learning. We then analyze the\nlearning behavior and characteristics of the model under varying scenarios to\ngauge the potential benefits of a similar mechanism in ANN. Our empirical\nresults suggest that having separate populations of neurons with personalized\nerror views can enable efficient learning under class imbalance and limited\ndata, and reduce the susceptibility to unintended shortcut strategies, leading\nto improved generalization. This work highlights the potential of translating\nthe learning machinery of the brain into the design of a new generation of ANNs\nand provides further credence to the argument that biologically inspired AI may\nhold the key to overcoming the shortcomings of ANNs."},
{"title": "Seeing in Words: Learning to Classify through Language Bottlenecks", "authors": ["Khalid Saifullah", "Yuxin Wen", "Jonas Geiping", "Micah Goldblum", "Tom Goldstein"], "url": "https://arxiv.org/abs/2307.00028", "abstract": "Neural networks for computer vision extract uninterpretable features despite\nachieving high accuracy on benchmarks. In contrast, humans can explain their\npredictions using succinct and intuitive descriptions. To incorporate\nexplainability into neural networks, we train a vision model whose feature\nrepresentations are text. We show that such a model can effectively classify\nImageNet images, and we discuss the challenges we encountered when training it."},
{"title": "CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion  Intensity Regulation", "authors": ["Yuhao Cui", "Xiongwei Wang", "Zhongzhou Zhao", "Wei Zhou", "Haiqing Chen"], "url": "https://arxiv.org/abs/2307.00020", "abstract": "Existing fine-grained intensity regulation methods rely on explicit control\nthrough predicted emotion probabilities. However, these high-level semantic\nprobabilities are often inaccurate and unsmooth at the phoneme level, leading\nto bias in learning. Especially when we attempt to mix multiple emotion\nintensities for specific phonemes, resulting in markedly reduced\ncontrollability and naturalness of the synthesis. To address this issue, we\npropose the CAScaded Explicit and Implicit coNtrol framework (CASEIN), which\nleverages accurate disentanglement of emotion manifolds from the reference\nspeech to learn the implicit representation at a lower semantic level. This\nrepresentation bridges the semantical gap between explicit probabilities and\nthe synthesis model, reducing bias in learning. In experiments, our CASEIN\nsurpasses existing methods in both controllability and naturalness. Notably, we\nare the first to achieve fine-grained control over the mixed intensity of\nmultiple emotions."},
{"title": "Personality Traits in Large Language Models", "authors": ["Mustafa Safdari", "Greg Serapio-Garc\u00eda", "Cl\u00e9ment Crepy", "Stephen Fitz", "Peter Romero", "Luning Sun", "Marwa Abdulhai", "Aleksandra Faust", "Maja Matari\u0107"], "url": "https://arxiv.org/abs/2307.00184", "abstract": "The advent of large language models (LLMs) has revolutionized natural\nlanguage processing, enabling the generation of coherent and contextually\nrelevant text. As LLMs increasingly power conversational agents, the\nsynthesized personality embedded in these models by virtue of their training on\nlarge amounts of human-generated data draws attention. Since personality is an\nimportant factor determining the effectiveness of communication, we present a\ncomprehensive method for administering validated psychometric tests and\nquantifying, analyzing, and shaping personality traits exhibited in text\ngenerated from widely-used LLMs. We find that: 1) personality simulated in the\noutputs of some LLMs (under specific prompting configurations) is reliable and\nvalid; 2) evidence of reliability and validity of LLM-simulated personality is\nstronger for larger and instruction fine-tuned models; and 3) personality in\nLLM outputs can be shaped along desired dimensions to mimic specific\npersonality profiles. We also discuss potential applications and ethical\nimplications of our measurement and shaping framework, especially regarding\nresponsible use of LLMs."},
{"title": "FFPDG: Fast, Fair and Private Data Generation", "authors": ["Weijie Xu", "Jinjin Zhao", "Francis Iannacci", "Bo Wang"], "url": "https://arxiv.org/abs/2307.00161", "abstract": "Generative modeling has been used frequently in synthetic data generation.\nFairness and privacy are two big concerns for synthetic data. Although Recent\nGAN [\\cite{goodfellow2014generative}] based methods show good results in\npreserving privacy, the generated data may be more biased. At the same time,\nthese methods require high computation resources. In this work, we design a\nfast, fair, flexible and private data generation method. We show the\neffectiveness of our method theoretically and empirically. We show that models\ntrained on data generated by the proposed method can perform well (in inference\nstage) on real application scenarios."},
{"title": "Stitched ViTs are Flexible Vision Backbones", "authors": ["Zizheng Pan", "Jing Liu", "Haoyu He", "Jianfei Cai", "Bohan Zhuang"], "url": "https://arxiv.org/abs/2307.00154", "abstract": "Large pretrained plain vision Transformers (ViTs) have been the workhorse for\nmany downstream tasks. However, existing works utilizing off-the-shelf ViTs are\ninefficient in terms of training and deployment, because adopting ViTs with\nindividual sizes requires separate training and is restricted by fixed\nperformance-efficiency trade-offs. In this paper, we are inspired by stitchable\nneural networks, which is a new framework that cheaply produces a single model\nthat covers rich subnetworks by stitching pretrained model families, supporting\ndiverse performance-efficiency trade-offs at runtime. Building upon this\nfoundation, we introduce SN-Netv2, a systematically improved model stitching\nframework to facilitate downstream task adaptation. Specifically, we first\npropose a Two-way stitching scheme to enlarge the stitching space. We then\ndesign a resource-constrained sampling strategy that takes into account the\nunderlying FLOPs distributions in the space for improved sampling. Finally, we\nobserve that learning stitching layers is a low-rank update, which plays an\nessential role on downstream tasks to stabilize training and ensure a good\nPareto frontier. With extensive experiments on ImageNet-1K, ADE20K,\nCOCO-Stuff-10K, NYUv2 and COCO-2017, SN-Netv2 demonstrates strong ability to\nserve as a flexible vision backbone, achieving great advantages in both\ntraining efficiency and adaptation. Code will be released at\n<a class=\"link-external link-https\" href=\"https://github.com/ziplab/SN-Netv2\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "Large Language Models (GPT) for automating feedback on programming  assignments", "authors": ["Maciej Pankiewicz", "Ryan S. Baker"], "url": "https://arxiv.org/abs/2307.00150", "abstract": "Addressing the challenge of generating personalized feedback for programming\nassignments is demanding due to several factors, like the complexity of code\nsyntax or different ways to correctly solve a task. In this experimental study,\nwe automated the process of feedback generation by employing OpenAI's GPT-3.5\nmodel to generate personalized hints for students solving programming\nassignments on an automated assessment platform. Students rated the usefulness\nof GPT-generated hints positively. The experimental group (with GPT hints\nenabled) relied less on the platform's regular feedback but performed better in\nterms of percentage of successful submissions across consecutive attempts for\ntasks, where GPT hints were enabled. For tasks where the GPT feedback was made\nunavailable, the experimental group needed significantly less time to solve\nassignments. Furthermore, when GPT hints were unavailable, students in the\nexperimental condition were initially less likely to solve the assignment\ncorrectly. This suggests potential over-reliance on GPT-generated feedback.\nHowever, students in the experimental condition were able to correct reasonably\nrapidly, reaching the same percentage correct after seven submission attempts.\nThe availability of GPT hints did not significantly impact students' affective\nstate."},
{"title": "A Personalized Household Assistive Robot that Learns and Creates New  Breakfast Options through Human-Robot Interaction", "authors": ["Ali Ayub", "Chrystopher L. Nehaniv", "Kerstin Dautenhahn"], "url": "https://arxiv.org/abs/2307.00114", "abstract": "For robots to assist users with household tasks, they must first learn about\nthe tasks from the users. Further, performing the same task every day, in the\nsame way, can become boring for the robot's user(s), therefore, assistive\nrobots must find creative ways to perform tasks in the household. In this\npaper, we present a cognitive architecture for a household assistive robot that\ncan learn personalized breakfast options from its users and then use the\nlearned knowledge to set up a table for breakfast. The architecture can also\nuse the learned knowledge to create new breakfast options over a longer period\nof time. The proposed cognitive architecture combines state-of-the-art\nperceptual learning algorithms, computational implementation of cognitive\nmodels of memory encoding and learning, a task planner for picking and placing\nobjects in the household, a graphical user interface (GUI) to interact with the\nuser and a novel approach for creating new breakfast options using the learned\nknowledge. The architecture is integrated with the Fetch mobile manipulator\nrobot and validated, as a proof-of-concept system evaluation in a large indoor\nenvironment with multiple kitchen objects. Experimental results demonstrate the\neffectiveness of our architecture to learn personalized breakfast options from\nthe user and generate new breakfast options never learned by the robot."},
{"title": "Ticket-BERT: Labeling Incident Management Tickets with Language Models", "authors": ["Zhexiong Liu", "Cris Benge", "Siduo Jiang"], "url": "https://arxiv.org/abs/2307.00108", "abstract": "An essential aspect of prioritizing incident tickets for resolution is\nefficiently labeling tickets with fine-grained categories. However, ticket data\nis often complex and poses several unique challenges for modern machine\nlearning methods: (1) tickets are created and updated either by machines with\npre-defined algorithms or by engineers with domain expertise that share\ndifferent protocols, (2) tickets receive frequent revisions that update ticket\nstatus by modifying all or parts of ticket descriptions, and (3) ticket\nlabeling is time-sensitive and requires knowledge updates and new labels per\nthe rapid software and hardware improvement lifecycle. To handle these issues,\nwe introduce Ticket- BERT which trains a simple yet robust language model for\nlabeling tickets using our proposed ticket datasets. Experiments demonstrate\nthe superiority of Ticket-BERT over baselines and state-of-the-art text\nclassifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT\nwith an active learning cycle and deploy it on the Microsoft IcM system, which\nenables the model to quickly finetune on newly-collected tickets with a few\nannotations."},
{"title": "VesselMorph: Domain-Generalized Retinal Vessel Segmentation via  Shape-Aware Representation", "authors": ["Dewei Hu", "Hao Li", "Han Liu", "Xing Yao", "Jiacheng Wang", "Ipek Oguz"], "url": "https://arxiv.org/abs/2307.00240", "abstract": "Due to the absence of a single standardized imaging protocol, domain shift\nbetween data acquired from different sites is an inherent property of medical\nimages and has become a major obstacle for large-scale deployment of\nlearning-based algorithms. For retinal vessel images, domain shift usually\npresents as the variation of intensity, contrast and resolution, while the\nbasic tubular shape of vessels remains unaffected. Thus, taking advantage of\nsuch domain-invariant morphological features can greatly improve the\ngeneralizability of deep models. In this study, we propose a method named\nVesselMorph which generalizes the 2D retinal vessel segmentation task by\nsynthesizing a shape-aware representation. Inspired by the traditional Frangi\nfilter and the diffusion tensor imaging literature, we introduce a\nHessian-based bipolar tensor field to depict the morphology of the vessels so\nthat the shape information is taken into account. We map the intensity image\nand the tensor field to a latent space for feature extraction. Then we fuse the\ntwo latent representations via a weight-balancing trick and feed the result to\na segmentation network. We evaluate on six public datasets of fundus and OCT\nangiography images from diverse patient populations. VesselMorph achieves\nsuperior generalization performance compared with competing methods in\ndifferent domain shift scenarios."},
{"title": "Queer People are People First: Deconstructing Sexual Identity  Stereotypes in Large Language Models", "authors": ["Harnoor Dhingra", "Preetiha Jayashanker", "Sayali Moghe", "Emma Strubell"], "url": "https://arxiv.org/abs/2307.00101", "abstract": "Large Language Models (LLMs) are trained primarily on minimally processed web\ntext, which exhibits the same wide range of social biases held by the humans\nwho created that content. Consequently, text generated by LLMs can\ninadvertently perpetuate stereotypes towards marginalized groups, like the\nLGBTQIA+ community. In this paper, we perform a comparative study of how LLMs\ngenerate text describing people with different sexual identities. Analyzing\nbias in the text generated by an LLM using regard score shows measurable bias\nagainst queer people. We then show that a post-hoc method based on\nchain-of-thought prompting using SHAP analysis can increase the regard of the\nsentence, representing a promising approach towards debiasing the output of\nLLMs in this setting."},
{"title": "Performance of ChatGPT on USMLE: Unlocking the Potential of Large  Language Models for AI-Assisted Medical Education", "authors": ["Prabin Sharma", "Kisan Thapa", "Prastab Dhakal", "Mala Deep Upadhaya", "Santosh Adhikari", "Salik Ram Khanal"], "url": "https://arxiv.org/abs/2307.00112", "abstract": "Artificial intelligence is gaining traction in more ways than ever before.\nThe popularity of language models and AI-based businesses has soared since\nChatGPT was made available to the general public via OpenAI. It is becoming\nincreasingly common for people to use ChatGPT both professionally and\npersonally. Considering the widespread use of ChatGPT and the reliance people\nplace on it, this study determined how reliable ChatGPT can be for answering\ncomplex medical and clinical questions. Harvard University gross anatomy along\nwith the United States Medical Licensing Examination (USMLE) questionnaire were\nused to accomplish the objective. The paper evaluated the obtained results\nusing a 2-way ANOVA and posthoc analysis. Both showed systematic covariation\nbetween format and prompt. Furthermore, the physician adjudicators\nindependently rated the outcome's accuracy, concordance, and insight. As a\nresult of the analysis, ChatGPT-generated answers were found to be more\ncontext-oriented and represented a better model for deductive reasoning than\nregular Google search results. Furthermore, ChatGPT obtained 58.8% on logical\nquestions and 60% on ethical questions. This means that the ChatGPT is\napproaching the passing range for logical questions and has crossed the\nthreshold for ethical questions. The paper believes ChatGPT and other language\nlearning models can be invaluable tools for e-learners; however, the study\nsuggests that there is still room to improve their accuracy. In order to\nimprove ChatGPT's performance in the future, further research is needed to\nbetter understand how it can answer different types of questions."},
{"title": "An Interpretable Constructive Algorithm for Incremental Random Weight  Neural Networks and Its Application", "authors": ["Jing Nan", "Wei Dai", "Guan Yuan", "Ping Zhou"], "url": "https://arxiv.org/abs/2307.00185", "abstract": "Incremental random weight neural networks (IRWNNs) have gained attention in\nview of its easy implementation and fast learning. However, a significant\ndrawback of IRWNNs is that the elationship between the hidden parameters\n(node)and the residual error (model performance) is difficult to be\ninterpreted. To address the above issue, this article proposes an interpretable\nconstructive algorithm (ICA) with geometric information constraint. First,\nbased on the geometric relationship between the hidden parameters and the\nresidual error, an interpretable geometric information constraint is proposed\nto randomly assign the hidden parameters. Meanwhile, a node pool strategy is\nemployed to obtain hidden parameters that is more conducive to convergence from\nhidden parameters satisfying the proposed constraint. Furthermore, the\nuniversal approximation property of the ICA is proved. Finally, a lightweight\nversion of ICA is presented for large-scale data modeling tasks. Experimental\nresults on six benchmark datasets and a numerical simulation dataset\ndemonstrate that the ICA outperforms other constructive algorithms in terms of\nmodeling speed, model accuracy, and model network structure. Besides, two\npractical industrial application case are used to validate the effectiveness of\nICA in practical applications."},
{"title": "Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns  Captured by Unmanned Aerial Systems", "authors": ["Uma Meleti", "Abolfazl Razi"], "url": "https://arxiv.org/abs/2307.00104", "abstract": "This research paper addresses the challenge of detecting obscured wildfires\n(when the fire flames are covered by trees, smoke, clouds, and other natural\nbarriers) in real-time using drones equipped only with RGB cameras. We propose\na novel methodology that employs semantic segmentation based on the temporal\nanalysis of smoke patterns in video sequences. Our approach utilizes an\nencoder-decoder architecture based on deep convolutional neural network\narchitecture with a pre-trained CNN encoder and 3D convolutions for decoding\nwhile using sequential stacking of features to exploit temporal variations. The\npredicted fire locations can assist drones in effectively combating forest\nfires and pinpoint fire retardant chemical drop on exact flame locations. We\napplied our method to a curated dataset derived from the FLAME2 dataset that\nincludes RGB video along with IR video to determine the ground truth. Our\nproposed method has a unique property of detecting obscured fire and achieves a\nDice score of 85.88%, while achieving a high precision of 92.47% and\nclassification accuracy of 90.67% on test data showing promising results when\ninspected visually. Indeed, our method outperforms other methods by a\nsignificant margin in terms of video-level fire classification as we obtained\nabout 100% accuracy using MobileNet+CBAM as the encoder backbone."},
{"title": "Image Matters: A New Dataset and Empirical Study for Multimodal  Hyperbole Detection", "authors": ["Huixuan Zhang", "Xiaojun Wan"], "url": "https://arxiv.org/abs/2307.00209", "abstract": "Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection\nof hyperbole is an important part of understanding human expression. There have\nbeen several studies on hyperbole detection, but most of which focus on text\nmodality only. However, with the development of social media, people can create\nhyperbolic expressions with various modalities, including text, images, videos,\netc. In this paper, we focus on multimodal hyperbole detection. We create a\nmultimodal detection dataset\\footnote{The dataset will be released to the\ncommunity.} from Weibo (a Chinese social media) and carry out some studies on\nit. We treat the text and image from a piece of weibo as two modalities and\nexplore the role of text and image for hyperbole detection. Different\npre-trained multimodal encoders are also evaluated on this downstream task to\nshow their performance. Besides, since this dataset is constructed from five\ndifferent topics, we also evaluate the cross-domain performance of different\nmodels. These studies can serve as a benchmark and point out the direction of\nfurther study on multimodal hyperbole detection."},
{"title": "THUIR2 at NTCIR-16 Session Search (SS) Task", "authors": ["Weihang Su", "Xiangsheng Li", "Yiqun Liu", "Min Zhang", "Shaoping Ma"], "url": "https://arxiv.org/abs/2307.00250", "abstract": "Our team(THUIR2) participated in both FOSS and POSS subtasks of the NTCIR-161\nSession Search (SS) Task. This paper describes our approaches and results. In\nthe FOSS subtask, we submit five runs using learning-to-rank and fine-tuned\npre-trained language models. We fine-tuned the pre-trained language model with\nad-hoc data and session information and assembled them by a learning-to-rank\nmethod. The assembled model achieves the best performance among all\nparticipants in the preliminary evaluation. In the POSS subtask, we used an\nassembled model which also achieves the best performance in the preliminary\nevaluation."},
{"title": "InstructEval: Systematic Evaluation of Instruction Selection Methods", "authors": ["Anirudh Ajith", "Chris Pan", "Mengzhou Xia", "Ameet Deshpande", "Karthik Narasimhan"], "url": "https://arxiv.org/abs/2307.00259", "abstract": "In-context learning (ICL) performs tasks by prompting a large language model\n(LLM) using an instruction and a small set of annotated examples called\ndemonstrations. Recent work has shown that the precise details of the inputs\nused in the prompt significantly impacts ICL, which has incentivized\ninstruction selection algorithms. The effect of instruction-choice however is\nseverely underexplored, with existing analyses being restricted to shallow\nsubsets of models and tasks, which limits the generalizability of their\ninsights. We develop an ICL evaluation suite to conduct a thorough assessment\nof these techniques. The suite includes 13 open-sourced LLMs of varying scales\nfrom 4 distinct model families and covers 9 different tasks, representing a\nrange of task types across 3 categories. In this work, we evaluate the relative\nperformance of 7 popular instruction selection methods using our benchmark over\nfive desiderata relevant to ICL. We discover that using curated\nmanually-written instructions and simple instructions without any task-specific\ndescriptions often elicits superior ICL performance than that of automatic\ninstruction-induction methods, pointing to a lack of generalizability among the\nlatter. We release our evaluation suite for benchmarking instruction selection\napproaches, and call for more rigorous and generalizable methods in this space."},
{"title": "General Part Assembly Planning", "authors": ["Yulong Li", "Andy Zeng", "Shuran Song"], "url": "https://arxiv.org/abs/2307.00206", "abstract": "Most successes in autonomous robotic assembly have been restricted to single\ntarget or category. We propose to investigate general part assembly, the task\nof creating novel target assemblies with unseen part shapes. To tackle the\nplanning of general part assembly, we present General Part Assembly Transformer\n(GPAT), a transformer based model architecture that accurately predicts part\nposes by inferring how each part shape corresponds to the target shape. Our\nexperiments on both 3D CAD models and real-world scans demonstrate GPAT's\ngeneralization abilities to novel and diverse target and part shapes. Project\nwebsite: <a class=\"link-external link-https\" href=\"https://general-part-assembly.github.io/\" rel=\"external noopener nofollow\">this https URL</a>"},
{"title": "Forward-Forward Algorithm for Hyperspectral Image Classification: A  Preliminary Study", "authors": ["Sidike Paheding", "Abel A. Reyes-Angulo"], "url": "https://arxiv.org/abs/2307.00231", "abstract": "The back-propagation algorithm has long been the de-facto standard in\noptimizing weights and biases in neural networks, particularly in cutting-edge\ndeep learning models. Its widespread adoption in fields like natural language\nprocessing, computer vision, and remote sensing has revolutionized automation\nin various tasks. The popularity of back-propagation stems from its ability to\nachieve outstanding performance in tasks such as classification, detection, and\nsegmentation. Nevertheless, back-propagation is not without its limitations,\nencompassing sensitivity to initial conditions, vanishing gradients,\noverfitting, and computational complexity. The recent introduction of a\nforward-forward algorithm (FFA), which computes local goodness functions to\noptimize network parameters, alleviates the dependence on substantial\ncomputational resources and the constant need for architectural scaling. This\nstudy investigates the application of FFA for hyperspectral image\nclassification. Experimental results and comparative analysis are provided with\nthe use of the traditional back-propagation algorithm. Preliminary results show\nthe potential behind FFA and its promises."},
{"title": "WaveMixSR: A Resource-efficient Neural Network for Image  Super-resolution", "authors": ["Pranav Jeevan", "Akella Srinidhi", "Pasunuri Prathiba", "Amit Sethi"], "url": "https://arxiv.org/abs/2307.00430", "abstract": "Image super-resolution research recently been dominated by transformer models\nwhich need higher computational resources than CNNs due to the quadratic\ncomplexity of self-attention. We propose a new neural network -- WaveMixSR --\nfor image super-resolution based on WaveMix architecture which uses a\n2D-discrete wavelet transform for spatial token-mixing. Unlike\ntransformer-based models, WaveMixSR does not unroll the image as a sequence of\npixels/patches. It uses the inductive bias of convolutions along with the\nlossless token-mixing property of wavelet transform to achieve higher\nperformance while requiring fewer resources and training data. We compare the\nperformance of our network with other state-of-the-art methods for image\nsuper-resolution. Our experiments show that WaveMixSR achieves competitive\nperformance in all datasets and reaches state-of-the-art performance in the\nBSD100 dataset on multiple super-resolution tasks. Our model is able to achieve\nthis performance using less training data and computational resources while\nmaintaining high parameter efficiency compared to current state-of-the-art\nmodels."},
{"title": "Sparsity aware generalization theory for deep neural networks", "authors": ["Ramchandran Muthukumar", "Jeremias Sulam"], "url": "https://arxiv.org/abs/2307.00426", "abstract": "Deep artificial neural networks achieve surprising generalization abilities\nthat remain poorly understood. In this paper, we present a new approach to\nanalyzing generalization for deep feed-forward ReLU networks that takes\nadvantage of the degree of sparsity that is achieved in the hidden layer\nactivations. By developing a framework that accounts for this reduced effective\nmodel size for each input sample, we are able to show fundamental trade-offs\nbetween sparsity and generalization. Importantly, our results make no strong\nassumptions about the degree of sparsity achieved by the model, and it improves\nover recent norm-based approaches. We illustrate our results numerically,\ndemonstrating non-vacuous bounds when coupled with data-dependent priors in\nspecific settings, even in over-parametrized models."},
{"title": "WavePaint: Resource-efficient Token-mixer for Self-supervised Inpainting", "authors": ["Pranav Jeevan", "Dharshan Sampath Kumar", "Amit Sethi"], "url": "https://arxiv.org/abs/2307.00407", "abstract": "Image inpainting, which refers to the synthesis of missing regions in an\nimage, can help restore occluded or degraded areas and also serve as a\nprecursor task for self-supervision. The current state-of-the-art models for\nimage inpainting are computationally heavy as they are based on transformer or\nCNN backbones that are trained in adversarial or diffusion settings. This paper\ndiverges from vision transformers by using a computationally-efficient\nWaveMix-based fully convolutional architecture -- WavePaint. It uses a\n2D-discrete wavelet transform (DWT) for spatial and multi-resolution\ntoken-mixing along with convolutional layers. The proposed model outperforms\nthe current state-of-the-art models for image inpainting on reconstruction\nquality while also using less than half the parameter count and considerably\nlower training and evaluation times. Our model even outperforms current\nGAN-based architectures in CelebA-HQ dataset without using an adversarially\ntrainable discriminator. Our work suggests that neural architectures that are\nmodeled after natural image priors require fewer parameters and computations to\nachieve generalization comparable to transformers."},
{"title": "ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models", "authors": ["Uddeshya Upadhyay", "Shyamgopal Karthik", "Massimiliano Mancini", "Zeynep Akata"], "url": "https://arxiv.org/abs/2307.00398", "abstract": "Large-scale vision-language models (VLMs) like CLIP successfully find\ncorrespondences between images and text. Through the standard deterministic\nmapping process, an image or a text sample is mapped to a single vector in the\nembedding space. This is problematic: as multiple samples (images or text) can\nabstract the same concept in the physical world, deterministic embeddings do\nnot reflect the inherent ambiguity in the embedding space. We propose ProbVLM,\na probabilistic adapter that estimates probability distributions for the\nembeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc\nmanner without needing large-scale datasets or computing. On four challenging\ndatasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the\nmulti-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify\nthe calibration of embedding uncertainties in retrieval tasks and show that\nProbVLM outperforms other methods. Furthermore, we propose active learning and\nmodel selection as two real-world downstream tasks for VLMs and show that the\nestimated uncertainty aids both tasks. Lastly, we present a novel technique for\nvisualizing the embedding distributions using a large-scale pre-trained latent\ndiffusion model."},
{"title": "CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular  Data Synthesis", "authors": ["Abdallah Alshantti", "Damiano Varagnolo", "Adil Rasheed", "Aria Rahmati", "Frank Westad"], "url": "https://arxiv.org/abs/2307.00384", "abstract": "Generative adversarial networks (GANs) have drawn considerable attention in\nrecent years for their proven capability in generating synthetic data which can\nbe utilized for multiple purposes. While GANs have demonstrated tremendous\nsuccesses in producing synthetic data samples that replicate the dynamics of\nthe original datasets, the validity of the synthetic data and the underlying\nprivacy concerns represent major challenges which are not sufficiently\naddressed. In this work, we design a cascaded tabular GAN framework (CasTGAN)\nfor generating realistic tabular data with a specific focus on the validity of\nthe output. In this context, validity refers to the the dependency between\nfeatures that can be found in the real data, but is typically misrepresented by\ntraditional generative models. Our key idea entails that employing a cascaded\narchitecture in which a dedicated generator samples each feature, the synthetic\noutput becomes more representative of the real data. Our experimental results\ndemonstrate that our model well captures the constraints and the correlations\nbetween the features of the real data, especially the high dimensional\ndatasets. Furthermore, we evaluate the risk of white-box privacy attacks on our\nmodel and subsequently show that applying some perturbations to the auxiliary\nlearners in CasTGAN increases the overall robustness of our model against\ntargeted attacks."},
{"title": "Minimizing Energy Consumption of Deep Learning Models by Energy-Aware  Training", "authors": ["Dario Lazzaro", "Antonio Emanuele Cin\u00e0", "Maura Pintor", "Ambra Demontis", "Battista Biggio", "Fabio Roli", "Marcello Pelillo"], "url": "https://arxiv.org/abs/2307.00368", "abstract": "Deep learning models undergo a significant increase in the number of\nparameters they possess, leading to the execution of a larger number of\noperations during inference. This expansion significantly contributes to higher\nenergy consumption and prediction latency. In this work, we propose EAT, a\ngradient-based algorithm that aims to reduce energy consumption during model\ntraining. To this end, we leverage a differentiable approximation of the\n$\\ell_0$ norm, and use it as a sparse penalty over the training loss. Through\nour experimental analysis conducted on three datasets and two deep neural\nnetworks, we demonstrate that our energy-aware training algorithm EAT is able\nto train networks with a better trade-off between classification performance\nand energy efficiency."},
{"title": "The future of human-centric eXplainable Artificial Intelligence (XAI) is  not post-hoc explanations", "authors": ["Vinitra Swamy", "Jibril Frej", "Tanja K\u00e4ser"], "url": "https://arxiv.org/abs/2307.00364", "abstract": "Explainable Artificial Intelligence (XAI) plays a crucial role in enabling\nhuman understanding and trust in deep learning systems, often defined as\ndetermining which features are most important to a model's prediction. As\nmodels get larger, more ubiquitous, and pervasive in aspects of daily life,\nexplainability is necessary to avoid or minimize adverse effects of model\nmistakes. Unfortunately, current approaches in human-centric XAI (e.g.\npredictive tasks in healthcare, education, or personalized ads) tend to rely on\na single explainer. This is a particularly concerning trend when considering\nthat recent work has identified systematic disagreement in explainability\nmethods when applied to the same points and underlying black-box models. In\nthis paper, we therefore present a call for action to address the limitations\nof current state-of-the-art explainers. We propose to shift from post-hoc\nexplainability to designing interpretable neural network architectures; moving\naway from approximation techniques in human-centric and high impact\napplications. We identify five needs of human-centric XAI (real-time, accurate,\nactionable, human-interpretable, and consistent) and propose two schemes for\ninterpretable-by-design neural network workflows (adaptive routing for\ninterpretable conditional computation and diagnostic benchmarks for iterative\nmodel learning). We postulate that the future of human-centric XAI is neither\nin explaining black-boxes nor in reverting to traditional, interpretable\nmodels, but in neural networks that are intrinsically interpretable."},
{"title": "A Comparative Study of Machine Learning Algorithms for Anomaly Detection  in Industrial Environments: Performance and Environmental Impact", "authors": ["\u00c1lvaro Huertas-Garc\u00eda", "Carlos Mart\u00ed-Gonz\u00e1lez", "Rub\u00e9n Garc\u00eda Maezo", "Alejandro Echeverr\u00eda Rey"], "url": "https://arxiv.org/abs/2307.00361", "abstract": "In the context of Industry 4.0, the use of artificial intelligence (AI) and\nmachine learning for anomaly detection is being hampered by high computational\nrequirements and associated environmental effects. This study seeks to address\nthe demands of high-performance machine learning models with environmental\nsustainability, contributing to the emerging discourse on 'Green AI.' An\nextensive variety of machine learning algorithms, coupled with various\nMultilayer Perceptron (MLP) configurations, were meticulously evaluated. Our\ninvestigation encapsulated a comprehensive suite of evaluation metrics,\ncomprising Accuracy, Area Under the Curve (AUC), Recall, Precision, F1 Score,\nKappa Statistic, Matthews Correlation Coefficient (MCC), and F1 Macro.\nSimultaneously, the environmental footprint of these models was gauged through\nconsiderations of time duration, CO2 equivalent, and energy consumption during\nthe training, cross-validation, and inference phases. Traditional machine\nlearning algorithms, such as Decision Trees and Random Forests, demonstrate\nrobust efficiency and performance. However, superior outcomes were obtained\nwith optimised MLP configurations, albeit with a commensurate increase in\nresource consumption. The study incorporated a multi-objective optimisation\napproach, invoking Pareto optimality principles, to highlight the trade-offs\nbetween a model's performance and its environmental impact. The insights\nderived underscore the imperative of striking a balance between model\nperformance, complexity, and environmental implications, thus offering valuable\ndirections for future work in the development of environmentally conscious\nmachine learning models for industrial applications."},
{"title": "Variation-aware Vision Transformer Quantization", "authors": ["Xijie Huang", "Zhiqiang Shen", "Kwang-Ting Cheng"], "url": "https://arxiv.org/abs/2307.00331", "abstract": "Despite the remarkable performance of Vision Transformers (ViTs) in various\nvisual tasks, the expanding computation and model size of ViTs have increased\nthe demand for improved efficiency during training and inference. To address\nthe heavy computation and parameter drawbacks, quantization is frequently\nstudied in the community as a representative model compression technique and\nhas seen extensive use on CNNs. However, due to the unique properties of CNNs\nand ViTs, the quantization applications on ViTs are still limited and\nunderexplored. In this paper, we identify the difficulty of ViT quantization on\nits unique variation behaviors, which differ from traditional CNN\narchitectures. The variations indicate the magnitude of the parameter\nfluctuations and can also measure outlier conditions. Moreover, the variation\nbehaviors reflect the various sensitivities to the quantization of each module.\nThe quantization sensitivity analysis and comparison of ViTs with CNNs help us\nlocate the underlying differences in variations. We also find that the\nvariations in ViTs cause training oscillations, bringing instability during\nquantization-aware training (QAT). Correspondingly, we solve the variation\nproblem with an efficient knowledge-distillation-based variation-aware\nquantization method. The multi-crop knowledge distillation scheme can\naccelerate and stabilize the training and alleviate the variation's influence\nduring QAT. We also proposed a module-dependent quantization scheme and a\nvariation-aware regularization term to suppress the oscillation of weights. On\nImageNet-1K, we obtain a 77.66% Top-1 accuracy on the extremely low-bit\nscenario of 2-bit Swin-T, outperforming the previous state-of-the-art quantized\nmodel by 3.35%."},
{"title": "When Synthetic Data Met Regulation", "authors": ["Georgi Ganev"], "url": "https://arxiv.org/abs/2307.00359", "abstract": "In this paper, we argue that synthetic data produced by Differentially\nPrivate generative models can be sufficiently anonymized and, therefore,\nanonymous data and regulatory compliant."},
{"title": "SHARCS: Shared Concept Space for Explainable Multimodal Learning", "authors": ["Gabriele Dominici", "Pietro Barbiero", "Lucie Charlotte Magister", "Pietro Li\u00f2", "Nikola Simidjievski"], "url": "https://arxiv.org/abs/2307.00316", "abstract": "Multimodal learning is an essential paradigm for addressing complex\nreal-world problems, where individual data modalities are typically\ninsufficient to accurately solve a given modelling task. While various deep\nlearning approaches have successfully addressed these challenges, their\nreasoning process is often opaque; limiting the capabilities for a principled\nexplainable cross-modal analysis and any domain-expert intervention. In this\npaper, we introduce SHARCS (SHARed Concept Space) -- a novel concept-based\napproach for explainable multimodal learning. SHARCS learns and maps\ninterpretable concepts from different heterogeneous modalities into a single\nunified concept-manifold, which leads to an intuitive projection of\nsemantically similar cross-modal concepts. We demonstrate that such an approach\ncan lead to inherently explainable task predictions while also improving\ndownstream predictive performance. Moreover, we show that SHARCS can operate\nand significantly outperform other approaches in practically significant\nscenarios, such as retrieval of missing modalities and cross-modal\nexplanations. Our approach is model-agnostic and easily applicable to different\ntypes (and number) of modalities, thus advancing the development of effective,\ninterpretable, and trustworthy multimodal approaches."},
{"title": "Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD", "authors": ["Anvith Thudi", "Hengrui Jia", "Casey Meehan", "Ilia Shumailov", "Nicolas Papernot"], "url": "https://arxiv.org/abs/2307.00310", "abstract": "Differentially private stochastic gradient descent (DP-SGD) is the canonical\nalgorithm for private deep learning. While it is known that its privacy\nanalysis is tight in the worst-case, several empirical results suggest that\nwhen training on common benchmark datasets, the models obtained leak\nsignificantly less privacy for many datapoints. In this paper, we develop a new\nanalysis for DP-SGD that captures the intuition that points with similar\nneighbors in the dataset enjoy better privacy than outliers. Formally, this is\ndone by modifying the per-step privacy analysis of DP-SGD to introduce a\ndependence on the distribution of model updates computed from a training\ndataset. We further develop a new composition theorem to effectively use this\nnew per-step analysis to reason about an entire training run. Put all together,\nour evaluation shows that this novel DP-SGD analysis allows us to now formally\nshow that DP-SGD leaks significantly less privacy for many datapoints. In\nparticular, we observe that correctly classified points obtain better privacy\nguarantees than misclassified points."},
{"title": "SysNoise: Exploring and Benchmarking Training-Deployment System  Inconsistency", "authors": ["Yan Wang", "Yuhang Li", "Ruihao Gong", "Aishan Liu", "Yanfei Wang", "Jian Hu", "Yongqiang Yao", "Yunchen Zhang", "Tianzi Xiao", "Fengwei Yu", "Xianglong Liu"], "url": "https://arxiv.org/abs/2307.00280", "abstract": "Extensive studies have shown that deep learning models are vulnerable to\nadversarial and natural noises, yet little is known about model robustness on\nnoises caused by different system implementations. In this paper, we for the\nfirst time introduce SysNoise, a frequently occurred but often overlooked noise\nin the deep learning training-deployment cycle. In particular, SysNoise happens\nwhen the source training system switches to a disparate target system in\ndeployments, where various tiny system mismatch adds up to a non-negligible\ndifference. We first identify and classify SysNoise into three categories based\non the inference stage; we then build a holistic benchmark to quantitatively\nmeasure the impact of SysNoise on 20+ models, comprehending image\nclassification, object detection, instance segmentation and natural language\nprocessing tasks. Our extensive experiments revealed that SysNoise could bring\ncertain impacts on model robustness across different tasks and common\nmitigations like data augmentation and adversarial training show limited\neffects on it. Together, our findings open a new research topic and we hope\nthis work will raise research attention to deep learning deployment systems\naccounting for model performance. We have open-sourced the benchmark and\nframework at <a class=\"link-external link-https\" href=\"https://modeltc.github.io/systemnoise_web\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D  Object Pose Estimation", "authors": ["Fabian Duffhauss", "Sebastian Koch", "Hanna Ziesche", "Ngo Anh Vien", "Gerhard Neumann"], "url": "https://arxiv.org/abs/2307.00306", "abstract": "Detecting objects and estimating their 6D poses is essential for automated\nsystems to interact safely with the environment. Most 6D pose estimators,\nhowever, rely on a single camera frame and suffer from occlusions and\nambiguities due to object symmetries. We overcome this issue by presenting a\nnovel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach\nefficiently fuses the RGB-D frames from multiple perspectives in a deep\nmulti-directional fusion network and predicts predefined keypoints for all\nobjects in the scene simultaneously. Based on the keypoints and an instance\nsemantic segmentation, we efficiently compute the 6D poses by least-squares\nfitting. To address the ambiguity issues for symmetric objects, we propose a\nnovel training procedure for symmetry-aware keypoint detection including a new\nobjective function. Our SyMFM6D network significantly outperforms the\nstate-of-the-art in both single-view and multi-view 6D pose estimation. We\nfurthermore show the effectiveness of our symmetry-aware training procedure and\ndemonstrate that our approach is robust towards inaccurate camera calibration\nand dynamic camera setups."},
{"title": "An End-to-End Multi-Module Audio Deepfake Generation System for ADD  Challenge 2023", "authors": ["Sheng Zhao", "Qilong Yuan", "Yibo Duan", "Zhuoyue Chen"], "url": "https://arxiv.org/abs/2307.00729", "abstract": "The task of synthetic speech generation is to generate language content from\na given text, then simulating fake human voice.The key factors that determine\nthe effect of synthetic speech generation mainly include speed of generation,\naccuracy of word segmentation, naturalness of synthesized speech, etc. This\npaper builds an end-to-end multi-module synthetic speech generation model,\nincluding speaker encoder, synthesizer based on Tacotron2, and vocoder based on\nWaveRNN. In addition, we perform a lot of comparative experiments on different\ndatasets and various model structures. Finally, we won the first place in the\nADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of\n44.97%."},
{"title": "Worth of knowledge in deep learning", "authors": ["Hao Xu", "Yuntian Chen", "Dongxiao Zhang"], "url": "https://arxiv.org/abs/2307.00712", "abstract": "Knowledge constitutes the accumulated understanding and experience that\nhumans use to gain insight into the world. In deep learning, prior knowledge is\nessential for mitigating shortcomings of data-driven models, such as data\ndependence, generalization ability, and compliance with constraints. To enable\nefficient evaluation of the worth of knowledge, we present a framework inspired\nby interpretable machine learning. Through quantitative experiments, we assess\nthe influence of data volume and estimation range on the worth of knowledge.\nOur findings elucidate the complex relationship between data and knowledge,\nincluding dependence, synergistic, and substitution effects. Our model-agnostic\nframework can be applied to a variety of common network architectures,\nproviding a comprehensive understanding of the role of prior knowledge in deep\nlearning models. It can also be used to improve the performance of informed\nmachine learning, as well as distinguish improper prior knowledge."},
{"title": "From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and  Privacy", "authors": ["Maanak Gupta", "CharanKumar Akiri", "Kshitiz Aryal", "Eli Parker", "Lopamudra Praharaj"], "url": "https://arxiv.org/abs/2307.00691", "abstract": "Undoubtedly, the evolution of Generative AI (GenAI) models has been the\nhighlight of digital transformation in the year 2022. As the different GenAI\nmodels like ChatGPT and Google Bard continue to foster their complexity and\ncapability, it's critical to understand its consequences from a cybersecurity\nperspective. Several instances recently have demonstrated the use of GenAI\ntools in both the defensive and offensive side of cybersecurity, and focusing\non the social, ethical and privacy implications this technology possesses. This\nresearch paper highlights the limitations, challenges, potential risks, and\nopportunities of GenAI in the domain of cybersecurity and privacy. The work\npresents the vulnerabilities of ChatGPT, which can be exploited by malicious\nusers to exfiltrate malicious information bypassing the ethical constraints on\nthe model. This paper demonstrates successful example attacks like Jailbreaks,\nreverse psychology, and prompt injection attacks on the ChatGPT. The paper also\ninvestigates how cyber offenders can use the GenAI tools in developing cyber\nattacks, and explore the scenarios where ChatGPT can be used by adversaries to\ncreate social engineering attacks, phishing attacks, automated hacking, attack\npayload generation, malware creation, and polymorphic malware. This paper then\nexamines defense techniques and uses GenAI tools to improve security measures,\nincluding cyber defense automation, reporting, threat intelligence, secure code\ngeneration and detection, attack identification, developing ethical guidelines,\nincidence response plans, and malware detection. We will also discuss the\nsocial, legal, and ethical implications of ChatGPT. In conclusion, the paper\nhighlights open challenges and future directions to make this GenAI secure,\nsafe, trustworthy, and ethical as the community understands its cybersecurity\nimpacts."},
{"title": "Hierarchical Pretraining for Biomedical Term Embeddings", "authors": ["Bryan Cai", "Sihang Zeng", "Yucong Lin", "Zheng Yuan", "Doudou Zhou", "Lu Tian"], "url": "https://arxiv.org/abs/2307.00266", "abstract": "Electronic health records (EHR) contain narrative notes that provide\nextensive details on the medical condition and management of patients. Natural\nlanguage processing (NLP) of clinical notes can use observed frequencies of\nclinical terms as predictive features for downstream applications such as\nclinical decision making and patient trajectory prediction. However, due to the\nvast number of highly similar and related clinical concepts, a more effective\nmodeling strategy is to represent clinical terms as semantic embeddings via\nrepresentation learning and use the low dimensional embeddings as feature\nvectors for predictive modeling. To achieve efficient representation,\nfine-tuning pretrained language models with biomedical knowledge graphs may\ngenerate better embeddings for biomedical terms than those from standard\nlanguage models alone. These embeddings can effectively discriminate synonymous\npairs of from those that are unrelated. However, they often fail to capture\ndifferent degrees of similarity or relatedness for concepts that are\nhierarchical in nature. To overcome this limitation, we propose HiPrBERT, a\nnovel biomedical term representation model trained on additionally complied\ndata that contains hierarchical structures for various biomedical terms. We\nmodify an existing contrastive loss function to extract information from these\nhierarchies. Our numerical experiments demonstrate that HiPrBERT effectively\nlearns the pair-wise distance from hierarchical information, resulting in a\nsubstantially more informative embeddings for further biomedical applications"},
{"title": "Intra- &amp; Extra-Source Exemplar-Based Style Synthesis for Improved Domain  Generalization", "authors": ["Yumeng Li", "Dan Zhang", "Margret Keuper", "Anna Khoreva"], "url": "https://arxiv.org/abs/2307.00648", "abstract": "The generalization with respect to domain shifts, as they frequently appear\nin applications such as autonomous driving, is one of the remaining big\nchallenges for deep learning models. Therefore, we propose an exemplar-based\nstyle synthesis pipeline to improve domain generalization in semantic\nsegmentation. Our method is based on a novel masked noise encoder for StyleGAN2\ninversion. The model learns to faithfully reconstruct the image, preserving its\nsemantic layout through noise prediction. Using the proposed masked noise\nencoder to randomize style and content combinations in the training set, i.e.,\nintra-source style augmentation (ISSA) effectively increases the diversity of\ntraining data and reduces spurious correlation. As a result, we achieve up to\n$12.4\\%$ mIoU improvements on driving-scene semantic segmentation under\ndifferent types of data shifts, i.e., changing geographic locations, adverse\nweather conditions, and day to night. ISSA is model-agnostic and\nstraightforwardly applicable with CNNs and Transformers. It is also\ncomplementary to other domain generalization techniques, e.g., it improves the\nrecent state-of-the-art solution RobustNet by $3\\%$ mIoU in Cityscapes to Dark\nZ\u00fcrich. In addition, we demonstrate the strong plug-n-play ability of the\nproposed style synthesis pipeline, which is readily usable for extra-source\nexemplars e.g., web-crawled images, without any retraining or fine-tuning.\nMoreover, we study a new use case to indicate neural network's generalization\ncapability by building a stylized proxy validation set. This application has\nsignificant practical sense for selecting models to be deployed in the\nopen-world environment. Our code is available at\n\\url{<a class=\"link-external link-https\" href=\"https://github.com/boschresearch/ISSA\" rel=\"external noopener nofollow\">this https URL</a>}."},
{"title": "DoReMi: Grounding Language Model by Detecting and Recovering from  Plan-Execution Misalignment", "authors": ["Yanjiang Guo", "Yen-Jen Wang", "Lihan Zha", "Zheyuan Jiang", "Jianyu Chen"], "url": "https://arxiv.org/abs/2307.00329", "abstract": "Large language models encode a vast amount of semantic knowledge and possess\nremarkable understanding and reasoning capabilities. Previous research has\nexplored how to ground language models in robotic tasks to ensure that the\nsequences generated by the language model are both logically correct and\npractically executable. However, low-level execution may deviate from the\nhigh-level plan due to environmental perturbations or imperfect controller\ndesign. In this paper, we propose DoReMi, a novel language model grounding\nframework that enables immediate Detection and Recovery from Misalignments\nbetween plan and execution. Specifically, during low-level skill execution, we\nuse a vision question answering (VQA) model to regularly detect plan-execution\nmisalignments. If certain misalignment occurs, our method will call the\nlanguage model to re-plan in order to recover from misalignments. Experiments\non various complex tasks including robot arms and humanoid robots demonstrate\nthat our method can lead to higher task success rates and shorter task\ncompletion times. Videos of DoReMi are available at\n<a class=\"link-external link-https\" href=\"https://sites.google.com/view/doremi-paper\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary  Directed Differential with Normalized Density and Self-Adaption", "authors": ["Hao Shu"], "url": "https://arxiv.org/abs/2307.00677", "abstract": "Density-based clustering could be the most popular clustering algorithm since\nit can identify clusters of arbitrary shape as long as different (high-density)\nclusters are separated by low-density regions. However, the requirement of the\nseparateness of clusters by low-density regions is not trivial since a\nhigh-density region might have different structures which should be clustered\ninto different groups. Such a situation demonstrates the main flaw of all\nprevious density-based clustering algorithms we have known--structures in a\nhigh-density cluster could not be detected. Therefore, this paper aims to\nprovide a density-based clustering scheme that not only has the ability\nprevious ones have but could also detect structures in a high-density region\nnot separated by low-density ones. The algorithm employs secondary directed\ndifferential, hierarchy, normalized density, as well as the self-adaption\ncoefficient, and thus is called Structure Detecting Cluster by Hierarchical\nSecondary Directed Differential with Normalized Density and Self-Adaption,\ndubbed by SDC-HSDD-NDSA for short. To illustrate its effectiveness, we run the\nalgorithm in several data sets. The results verify its validity in structure\ndetection, robustness over noises, as well as independence of granularities,\nand demonstrate that it could outperform previous ones. The Python code of the\npaper could be found on <a class=\"link-external link-https\" href=\"https://github.com/Hao-B-Shu/SDC-HSDD-NDSA\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "Solving Linear Inverse Problems Provably via Posterior Sampling with  Latent Diffusion Models", "authors": ["Litu Rout", "Negin Raoof", "Giannis Daras", "Constantine Caramanis", "Alexandros G. Dimakis", "Sanjay Shakkottai"], "url": "https://arxiv.org/abs/2307.00619", "abstract": "We present the first framework to solve linear inverse problems leveraging\npre-trained latent diffusion models. Previously proposed algorithms (such as\nDPS and DDRM) only apply to pixel-space diffusion models. We theoretically\nanalyze our algorithm showing provable sample recovery in a linear model\nsetting. The algorithmic insight obtained from our analysis extends to more\ngeneral settings often considered in practice. Experimentally, we outperform\npreviously proposed posterior sampling algorithms in a wide variety of problems\nincluding random inpainting, block inpainting, denoising, deblurring,\ndestriping, and super-resolution."},
{"title": "Effects of Explanation Specificity on Passengers in Autonomous Driving", "authors": ["Daniel Omeiza", "Raunak Bhattacharyya", "Nick Hawes", "Marina Jirotka", "Lars Kunze"], "url": "https://arxiv.org/abs/2307.00633", "abstract": "The nature of explanations provided by an explainable AI algorithm has been a\ntopic of interest in the explainable AI and human-computer interaction\ncommunity. In this paper, we investigate the effects of natural language\nexplanations' specificity on passengers in autonomous driving. We extended an\nexisting data-driven tree-based explainer algorithm by adding a rule-based\noption for explanation generation. We generated auditory natural language\nexplanations with different levels of specificity (abstract and specific) and\ntested these explanations in a within-subject user study (N=39) using an\nimmersive physical driving simulation setup. Our results showed that both\nabstract and specific explanations had similar positive effects on passengers'\nperceived safety and the feeling of anxiety. However, the specific explanations\ninfluenced the desire of passengers to takeover driving control from the\nautonomous vehicle (AV), while the abstract explanations did not. We conclude\nthat natural language auditory explanations are useful for passengers in\nautonomous driving, and their specificity levels could influence how much\nin-vehicle participants would wish to be in control of the driving activity."},
{"title": "The Forward-Forward Algorithm as a feature extractor for skin lesion  classification: A preliminary study", "authors": ["Abel Reyes-Angulo", "Sidike Paheding"], "url": "https://arxiv.org/abs/2307.00617", "abstract": "Skin cancer, a deadly form of cancer, exhibits a 23\\% survival rate in the\nUSA with late diagnosis. Early detection can significantly increase the\nsurvival rate, and facilitate timely treatment. Accurate biomedical image\nclassification is vital in medical analysis, aiding clinicians in disease\ndiagnosis and treatment. Deep learning (DL) techniques, such as convolutional\nneural networks and transformers, have revolutionized clinical decision-making\nautomation. However, computational cost and hardware constraints limit the\nimplementation of state-of-the-art DL architectures. In this work, we explore a\nnew type of neural network that does not need backpropagation (BP), namely the\nForward-Forward Algorithm (FFA), for skin lesion classification. While FFA is\nclaimed to use very low-power analog hardware, BP still tends to be superior in\nterms of classification accuracy. In addition, our experimental results suggest\nthat the combination of FFA and BP can be a better alternative to achieve a\nmore accurate prediction."},
{"title": "BioCPT: Contrastive Pre-trained Transformers with Large-scale PubMed  Search Logs for Zero-shot Biomedical Information Retrieval", "authors": ["Qiao Jin", "Won Kim", "Qingyu Chen", "Donald C. Comeau", "Lana Yeganova", "John Wilbur", "Zhiyong Lu"], "url": "https://arxiv.org/abs/2307.00589", "abstract": "Information retrieval (IR) is essential in biomedical knowledge acquisition\nand clinical decision support. While recent progress has shown that language\nmodel encoders perform better semantic retrieval, training such models requires\nabundant query-article annotations that are difficult to obtain in biomedicine.\nAs a result, most biomedical IR systems only conduct lexical matching. In\nresponse, we introduce BioCPT, a first-of-its-kind Contrastively Pre-trained\nTransformer model for zero-shot biomedical IR. To train BioCPT, we collected an\nunprecedented scale of 255 million user click logs from PubMed. With such data,\nwe use contrastive learning to train a pair of closely-integrated retriever and\nre-ranker. Experimental results show that BioCPT sets new state-of-the-art\nperformance on five biomedical IR tasks, outperforming various baselines\nincluding much larger models such as GPT-3-sized cpt-text-XL. In addition,\nBioCPT also generates better biomedical article and sentence representations\nfor semantic evaluations. As such, BioCPT can be readily applied to various\nreal-world biomedical IR tasks. BioCPT API and code are publicly available at\n<a class=\"link-external link-https\" href=\"https://github.com/ncbi/BioCPT\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "Adaptive reinforcement learning of multi-agent ethically-aligned  behaviours: the QSOM and QDSOM algorithms", "authors": ["R\u00e9my Chaput", "Olivier Boissier", "Mathieu Guillermin"], "url": "https://arxiv.org/abs/2307.00552", "abstract": "The numerous deployed Artificial Intelligence systems need to be aligned with\nour ethical considerations. However, such ethical considerations might change\nas time passes: our society is not fixed, and our social mores evolve. This\nmakes it difficult for these AI systems; in the Machine Ethics field\nespecially, it has remained an under-studied challenge. In this paper, we\npresent two algorithms, named QSOM and QDSOM, which are able to adapt to\nchanges in the environment, and especially in the reward function, which\nrepresents the ethical considerations that we want these systems to be aligned\nwith. They associate the well-known Q-Table to (Dynamic) Self-Organizing Maps\nto handle the continuous and multi-dimensional state and action spaces. We\nevaluate them on a use-case of multi-agent energy repartition within a small\nSmart Grid neighborhood, and prove their ability to adapt, and their higher\nperformance compared to baseline Reinforcement Learning algorithms."},
{"title": "RH20T: A Robotic Dataset for Learning Diverse Skills in One-Shot", "authors": ["Hao-Shu Fang", "Hongjie Fang", "Zhenyu Tang", "Jirong Liu", "Junbo Wang", "Haoyi Zhu", "Cewu Lu"], "url": "https://arxiv.org/abs/2307.00595", "abstract": "A key challenge in robotic manipulation in open domains is how to acquire\ndiverse and generalizable skills for robots. Recent research in one-shot\nimitation learning has shown promise in transferring trained policies to new\ntasks based on demonstrations. This feature is attractive for enabling robots\nto acquire new skills and improving task and motion planning. However, due to\nlimitations in the training dataset, the current focus of the community has\nmainly been on simple cases, such as push or pick-place tasks, relying solely\non visual guidance. In reality, there are many complex skills, some of which\nmay even require both visual and tactile perception to solve. This paper aims\nto unlock the potential for an agent to generalize to hundreds of real-world\nskills with multi-modal perception. To achieve this, we have collected a\ndataset comprising over 110,000 \\emph{contact-rich} robot manipulation\nsequences across diverse skills, contexts, robots, and camera viewpoints, all\ncollected \\emph{in the real world}. Each sequence in the dataset includes\nvisual, force, audio, and action information, along with a corresponding human\ndemonstration video. We have invested significant efforts in calibrating all\nthe sensors and ensuring a high-quality dataset. The dataset is made publicly\navailable at <a class=\"link-external link-http\" href=\"http://rh20t.github.io\" rel=\"external noopener nofollow\">this http URL</a>"},
{"title": "Defending Against Malicious Behaviors in Federated Learning with  Blockchain", "authors": ["Nanqing Dong", "Zhipeng Wang", "Jiahao Sun", "Michael Kampffmeyer", "Yizhe Wen", "Shuoying Zhang", "William Knottenbelt", "Eric Xing"], "url": "https://arxiv.org/abs/2307.00543", "abstract": "In the era of deep learning, federated learning (FL) presents a promising\napproach that allows multi-institutional data owners, or clients, to\ncollaboratively train machine learning models without compromising data\nprivacy. However, most existing FL approaches rely on a centralized server for\nglobal model aggregation, leading to a single point of failure. This makes the\nsystem vulnerable to malicious attacks when dealing with dishonest clients. In\nthis work, we address this problem by proposing a secure and reliable FL system\nbased on blockchain and distributed ledger technology. Our system incorporates\na peer-to-peer voting mechanism and a reward-and-slash mechanism, which are\npowered by on-chain smart contracts, to detect and deter malicious behaviors.\nBoth theoretical and empirical analyses are presented to demonstrate the\neffectiveness of the proposed approach, showing that our framework is robust\nagainst malicious client-side behaviors."},
{"title": "Collaborative Policy Learning for Dynamic Scheduling Tasks in  Cloud-Edge-Terminal IoT Networks Using Federated Reinforcement Learning", "authors": ["Do-Yup Kim", "Da-Eun Lee", "Ji-Wan Kim", "Hyun-Suk Lee"], "url": "https://arxiv.org/abs/2307.00541", "abstract": "In this paper, we examine cloud-edge-terminal IoT networks, where edges\nundertake a range of typical dynamic scheduling tasks. In these IoT networks, a\ncentral policy for each task can be constructed at a cloud server. The central\npolicy can be then used by the edges conducting the task, thereby mitigating\nthe need for them to learn their own policy from scratch. Furthermore, this\ncentral policy can be collaboratively learned at the cloud server by\naggregating local experiences from the edges, thanks to the hierarchical\narchitecture of the IoT networks. To this end, we propose a novel collaborative\npolicy learning framework for dynamic scheduling tasks using federated\nreinforcement learning. For effective learning, our framework adaptively\nselects the tasks for collaborative learning in each round, taking into account\nthe need for fairness among tasks. In addition, as a key enabler of the\nframework, we propose an edge-agnostic policy structure that enables the\naggregation of local policies from different edges. We then provide the\nconvergence analysis of the framework. Through simulations, we demonstrate that\nour proposed framework significantly outperforms the approaches without\ncollaborative policy learning. Notably, it accelerates the learning speed of\nthe policies and allows newly arrived edges to adapt to their tasks more\neasily."},
{"title": "Graph Neural Network based Log Anomaly Detection and Explanation", "authors": ["Zhong Li", "Jiayang Shi", "Matthijs van Leeuwen"], "url": "https://arxiv.org/abs/2307.00527", "abstract": "Event logs are widely used to record the status of high-tech systems, making\nlog anomaly detection important for monitoring those systems. Most existing log\nanomaly detection methods take a log event count matrix or log event sequences\nas input, exploiting quantitative and/or sequential relationships between log\nevents to detect anomalies. Unfortunately, only considering quantitative or\nsequential relationships may result in many false positives and/or false\nnegatives. To alleviate this problem, we propose a graph-based method for\nunsupervised log anomaly detection, dubbed Logs2Graphs, which first converts\nevent logs into attributed, directed, and weighted graphs, and then leverages\ngraph neural networks to perform graph-level anomaly detection. Specifically,\nwe introduce One-Class Digraph Inception Convolutional Networks, abbreviated as\nOCDiGCN, a novel graph neural network model for detecting graph-level anomalies\nin a collection of attributed, directed, and weighted graphs. By coupling the\ngraph representation and anomaly detection steps, OCDiGCN can learn a\nrepresentation that is especially suited for anomaly detection, resulting in a\nhigh detection accuracy. Importantly, for each identified anomaly, we\nadditionally provide a small subset of nodes that play a crucial role in\nOCDiGCN's prediction as explanations, which can offer valuable cues for\nsubsequent root cause diagnosis. Experiments on five benchmark datasets show\nthat Logs2Graphs performs at least on par state-of-the-art log anomaly\ndetection methods on simple datasets while largely outperforming\nstate-of-the-art log anomaly detection methods on complicated datasets."},
{"title": "LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance", "authors": ["Linoy Tsaban", "Apolin\u00e1rio Passos"], "url": "https://arxiv.org/abs/2307.00522", "abstract": "Recent large-scale text-guided diffusion models provide powerful\nimage-generation capabilities. Currently, a significant effort is given to\nenable the modification of these images using text only as means to offer\nintuitive and versatile editing. However, editing proves to be difficult for\nthese generative models due to the inherent nature of editing techniques, which\ninvolves preserving certain content from the original image. Conversely, in\ntext-based models, even minor modifications to the text prompt frequently\nresult in an entirely distinct result, making attaining one-shot generation\nthat accurately corresponds to the users intent exceedingly challenging. In\naddition, to edit a real image using these state-of-the-art tools, one must\nfirst invert the image into the pre-trained models domain - adding another\nfactor affecting the edit quality, as well as latency. In this exploratory\nreport, we propose LEDITS - a combined lightweight approach for real-image\nediting, incorporating the Edit Friendly DDPM inversion technique with Semantic\nGuidance, thus extending Semantic Guidance to real image editing, while\nharnessing the editing capabilities of DDPM inversion as well. This approach\nachieves versatile edits, both subtle and extensive as well as alterations in\ncomposition and style, while requiring no optimization nor extensions to the\narchitecture."},
{"title": "Challenges in Domain-Specific Abstractive Summarization and How to  Overcome them", "authors": ["Anum Afzal", "Juraj Vladika", "Daniel Braun", "Florian Matthes"], "url": "https://arxiv.org/abs/2307.00963", "abstract": "Large Language Models work quite well with general-purpose data and many\ntasks in Natural Language Processing. However, they show several limitations\nwhen used for a task such as domain-specific abstractive text summarization.\nThis paper identifies three of those limitations as research problems in the\ncontext of abstractive text summarization: 1) Quadratic complexity of\ntransformer-based models with respect to the input text length; 2) Model\nHallucination, which is a model's ability to generate factually incorrect text;\nand 3) Domain Shift, which happens when the distribution of the model's\ntraining and test corpus is not the same. Along with a discussion of the open\nresearch questions, this paper also provides an assessment of existing\nstate-of-the-art techniques relevant to domain-specific text summarization to\naddress the research gaps."},
{"title": "Neural Architecture Transfer 2: A Paradigm for Improving Efficiency in  Multi-Objective Neural Architecture Search", "authors": ["Simone Sarti", "Eugenio Lomurno", "Matteo Matteucci"], "url": "https://arxiv.org/abs/2307.00960", "abstract": "Deep learning is increasingly impacting various aspects of contemporary\nsociety. Artificial neural networks have emerged as the dominant models for\nsolving an expanding range of tasks. The introduction of Neural Architecture\nSearch (NAS) techniques, which enable the automatic design of task-optimal\nnetworks, has led to remarkable advances. However, the NAS process is typically\nassociated with long execution times and significant computational resource\nrequirements. Once-For-All (OFA) and its successor, Once-For-All-2 (OFAv2),\nhave been developed to mitigate these challenges. While maintaining exceptional\nperformance and eliminating the need for retraining, they aim to build a single\nsuper-network model capable of directly extracting sub-networks satisfying\ndifferent constraints. Neural Architecture Transfer (NAT) was developed to\nmaximise the effectiveness of extracting sub-networks from a super-network. In\nthis paper, we present NATv2, an extension of NAT that improves multi-objective\nsearch algorithms applied to dynamic super-network architectures. NATv2\nachieves qualitative improvements in the extractable sub-networks by exploiting\nthe improved super-networks generated by OFAv2 and incorporating new policies\nfor initialisation, pre-processing and updating its networks archive. In\naddition, a post-processing pipeline based on fine-tuning is introduced.\nExperimental results show that NATv2 successfully improves NAT and is highly\nrecommended for investigating high-performance architectures with a minimal\nnumber of parameters."},
{"title": "Towards Real Smart Apps: Investigating Human-AI Interactions in  Smartphone On-Device AI Apps", "authors": ["Jason Ching Yuen Siu", "Jieshan Chen", "Yujin Huang", "Zhenchang Xing", "Chunyang Chen"], "url": "https://arxiv.org/abs/2307.00756", "abstract": "With the emergence of deep learning techniques, smartphone apps are now\nembedded on-device AI features for enabling advanced tasks like speech\ntranslation, to attract users and increase market competitiveness. A good\ninteraction design is important to make an AI feature usable and\nunderstandable. However, AI features have their unique challenges like\nsensitiveness to the input, dynamic behaviours and output uncertainty. Existing\nguidelines and tools either do not cover AI features or consider mobile apps\nwhich are confirmed by our informal interview with professional designers. To\naddress these issues, we conducted the first empirical study to explore\nuser-AI-interaction in mobile apps. We aim to understand the status of\non-device AI usage by investigating 176 AI apps from 62,822 apps. We identified\n255 AI features and summarised 759 implementations into three primary\ninteraction pattern types. We further implemented our findings into a\nmulti-faceted search-enabled gallery. The results of the user study demonstrate\nthe usefulness of our findings."},
{"title": "Learning Differentiable Logic Programs for Abstract Visual Reasoning", "authors": ["Hikaru Shindo", "Viktor Pfanschilling", "Devendra Singh Dhami", "Kristian Kersting"], "url": "https://arxiv.org/abs/2307.00928", "abstract": "Visual reasoning is essential for building intelligent agents that understand\nthe world and perform problem-solving beyond perception. Differentiable forward\nreasoning has been developed to integrate reasoning with gradient-based machine\nlearning paradigms. However, due to the memory intensity, most existing\napproaches do not bring the best of the expressivity of first-order logic,\nexcluding a crucial ability to solve abstract visual reasoning, where agents\nneed to perform reasoning by using analogies on abstract concepts in different\nscenarios. To overcome this problem, we propose NEUro-symbolic Message-pAssiNg\nreasoNer (NEUMANN), which is a graph-based differentiable forward reasoner,\npassing messages in a memory-efficient manner and handling structured programs\nwith functors. Moreover, we propose a computationally-efficient structure\nlearning algorithm to perform explanatory program induction on complex visual\nscenes. To evaluate, in addition to conventional visual reasoning tasks, we\npropose a new task, visual reasoning behind-the-scenes, where agents need to\nlearn abstract programs and then answer queries by imagining scenes that are\nnot observed. We empirically demonstrate that NEUMANN solves visual reasoning\ntasks efficiently, outperforming neural, symbolic, and neuro-symbolic\nbaselines."},
{"title": "Automatic Design of Semantic Similarity Ensembles Using Grammatical  Evolution", "authors": ["Jorge Martinez-Gil"], "url": "https://arxiv.org/abs/2307.00925", "abstract": "Semantic similarity measures are widely used in natural language processing\nto catalyze various computer-related tasks. However, no single semantic\nsimilarity measure is the most appropriate for all tasks, and researchers often\nuse ensemble strategies to ensure performance. This research work proposes a\nmethod for automatically designing semantic similarity ensembles. In fact, our\nproposed method uses grammatical evolution, for the first time, to\nautomatically select and aggregate measures from a pool of candidates to create\nan ensemble that maximizes correlation to human judgment. The method is\nevaluated on several benchmark datasets and compared to state-of-the-art\nensembles, showing that it can significantly improve similarity assessment\naccuracy and outperform existing methods in some cases. As a result, our\nresearch demonstrates the potential of using grammatical evolution to\nautomatically compare text and prove the benefits of using ensembles for\nsemantic similarity tasks."},
{"title": "Achieving Stable Training of Reinforcement Learning Agents in Bimodal  Environments through Batch Learning", "authors": ["E. Hurwitz", "N. Peace", "G. Cevora"], "url": "https://arxiv.org/abs/2307.00923", "abstract": "Bimodal, stochastic environments present a challenge to typical Reinforcement\nLearning problems. This problem is one that is surprisingly common in real\nworld applications, being particularly applicable to pricing problems. In this\npaper we present a novel learning approach to the tabular Q-learning algorithm,\ntailored to tackling these specific challenges by using batch updates. A\nsimulation of pricing problem is used as a testbed to compare a typically\nupdated agent with a batch learning agent. The batch learning agents are shown\nto be both more effective than the typically-trained agents, and to be more\nresilient to the fluctuations in a large stochastic environment. This work has\na significant potential to enable practical, industrial deployment of\nReinforcement Learning in the context of pricing and others."},
{"title": "OpenAPMax: Abnormal Patterns-based Model for Real-World Alzheimer's  Disease Diagnosis", "authors": ["Yunyou Huang", "Xianglong Guan", "Xiangjiang Lu", "Xiaoshuang Liang", "Xiuxia Miao", "Jiyue Xie", "Wenjing Liu", "Li Ma", "Suqin Tang", "Zhifei Zhang", "Jianfeng Zhan"], "url": "https://arxiv.org/abs/2307.00936", "abstract": "Alzheimer's disease (AD) cannot be reversed, but early diagnosis will\nsignificantly benefit patients' medical treatment and care. In recent works, AD\ndiagnosis has the primary assumption that all categories are known a prior -- a\nclosed-set classification problem, which contrasts with the open-set\nrecognition problem. This assumption hinders the application of the model in\nnatural clinical settings. Although many open-set recognition technologies have\nbeen proposed in other fields, they are challenging to use for AD diagnosis\ndirectly since 1) AD is a degenerative disease of the nervous system with\nsimilar symptoms at each stage, and it is difficult to distinguish from its\npre-state, and 2) diversified strategies for AD diagnosis are challenging to\nmodel uniformly. In this work, inspired by the concerns of clinicians during\ndiagnosis, we propose an open-set recognition model, OpenAPMax, based on the\nanomaly pattern to address AD diagnosis in real-world settings. OpenAPMax first\nobtains the abnormal pattern of each patient relative to each known category\nthrough statistics or a literature search, clusters the patients' abnormal\npattern, and finally, uses extreme value theory (EVT) to model the distance\nbetween each patient's abnormal pattern and the center of their category and\nmodify the classification probability. We evaluate the performance of the\nproposed method with recent open-set recognition, where we obtain\nstate-of-the-art results."},
{"title": "Node-weighted Graph Convolutional Network for Depression Detection in  Transcribed Clinical Interviews", "authors": ["Sergio Burdisso", "Esa\u00fa Villatoro-Tello", "Srikanth Madikeri", "Petr Motlicek"], "url": "https://arxiv.org/abs/2307.00920", "abstract": "We propose a simple approach for weighting self-connecting edges in a Graph\nConvolutional Network (GCN) and show its impact on depression detection from\ntranscribed clinical interviews. To this end, we use a GCN for modeling\nnon-consecutive and long-distance semantics to classify the transcriptions into\ndepressed or control subjects. The proposed method aims to mitigate the\nlimiting assumptions of locality and the equal importance of self-connections\nvs. edges to neighboring nodes in GCNs, while preserving attractive features\nsuch as low computational cost, data agnostic, and interpretability\ncapabilities. We perform an exhaustive evaluation in two benchmark datasets.\nResults show that our approach consistently outperforms the vanilla GCN model\nas well as previously reported results, achieving an F1=0.84% on both datasets.\nFinally, a qualitative analysis illustrates the interpretability capabilities\nof the proposed approach and its alignment with previous findings in\npsychology."},
{"title": "Contextual Prompt Learning for Vision-Language Understanding", "authors": ["Koustava Goswami", "Srikrishna Karanam", "Joseph K J", "Prateksha Udhayanan", "Balaji Vasan Srinivasan"], "url": "https://arxiv.org/abs/2307.00910", "abstract": "Recent advances in multimodal learning has resulted in powerful\nvision-language models, whose representations are generalizable across a\nvariety of downstream tasks. Recently, their generalizability has been further\nextended by incorporating trainable prompts, borrowed from the natural language\nprocessing literature. While such prompt learning techniques have shown\nimpressive results, we identify that these prompts are trained based on global\nimage features which limits itself in two aspects: First, by using global\nfeatures, these prompts could be focusing less on the discriminative foreground\nimage, resulting in poor generalization to various out-of-distribution test\ncases. Second, existing work weights all prompts equally whereas our intuition\nis that these prompts are more specific to the type of the image. We address\nthese issues with as part of our proposed Contextual Prompt Learning (CoPL)\nframework, capable of aligning the prompts to the localized features of the\nimage. Our key innovations over earlier works include using local image\nfeatures as part of the prompt learning process, and more crucially, learning\nto weight these prompts based on local features that are appropriate for the\ntask at hand. This gives us dynamic prompts that are both aligned to local\nimage features as well as aware of local contextual relationships. Our\nextensive set of experiments on a variety of standard and few-shot datasets\nshow that our method produces substantially improved performance when compared\nto the current state of the art methods. We also demonstrate both few-shot and\nout-of-distribution performance to establish the utility of learning dynamic\nprompts that are aligned to local image features."},
{"title": "Fixing confirmation bias in feature attribution methods via semantic  match", "authors": ["Giovanni Cin\u00e0", "Daniel Fernandez-Llaneza", "Nishant Mishra", "Tabea E. R\u00f6ber", "Sandro Pezzelle", "Iacer Calixto", "Rob Goedhart", "\u015e. \u0130lker Birbil"], "url": "https://arxiv.org/abs/2307.00897", "abstract": "Feature attribution methods have become a staple method to disentangle the\ncomplex behavior of black box models. Despite their success, some scholars have\nargued that such methods suffer from a serious flaw: they do not allow a\nreliable interpretation in terms of human concepts. Simply put, visualizing an\narray of feature contributions is not enough for humans to conclude something\nabout a model's internal representations, and confirmation bias can trick users\ninto false beliefs about model behavior. We argue that a structured approach is\nrequired to test whether our hypotheses on the model are confirmed by the\nfeature attributions. This is what we call the \"semantic match\" between human\nconcepts and (sub-symbolic) explanations. Building on the conceptual framework\nput forward in Cin\u00e0 et al. [2023], we propose a structured approach to\nevaluate semantic match in practice. We showcase the procedure in a suite of\nexperiments spanning tabular and image data, and show how the assessment of\nsemantic match can give insight into both desirable (e.g., focusing on an\nobject relevant for prediction) and undesirable model behaviors (e.g., focusing\non a spurious correlation). We couple our experimental results with an analysis\non the metrics to measure semantic match, and argue that this approach\nconstitutes the first step towards resolving the issue of confirmation bias in\nXAI."},
{"title": "OpenSiteRec: An Open Dataset for Site Recommendation", "authors": ["Xinhang Li", "Xiangyu Zhao", "Yejing Wang", "Yu Liu", "Yong Li", "Cheng Long", "Yong Zhang", "Chunxiao Xing"], "url": "https://arxiv.org/abs/2307.00856", "abstract": "As a representative information retrieval task, site recommendation, which\naims at predicting the optimal sites for a brand or an institution to open new\nbranches in an automatic data-driven way, is beneficial and crucial for brand\ndevelopment in modern business. However, there is no publicly available dataset\nso far and most existing approaches are limited to an extremely small scope of\nbrands, which seriously hinders the research on site recommendation. Therefore,\nwe collect, construct and release an open comprehensive dataset, namely\nOpenSiteRec, to facilitate and promote the research on site recommendation.\nSpecifically, OpenSiteRec leverages a heterogeneous graph schema to represent\nvarious types of real-world entities and relations in four international\nmetropolises. To evaluate the performance of the existing general methods on\nthe site recommendation task, we conduct benchmarking experiments of several\nrepresentative recommendation models on OpenSiteRec. Furthermore, we also\nhighlight the potential application directions to demonstrate the wide\napplicability of OpenSiteRec. We believe that our OpenSiteRec dataset is\nsignificant and anticipated to encourage the development of advanced methods\nfor site recommendation. OpenSiteRec is available online at\n<a class=\"link-external link-https\" href=\"https://OpenSiteRec.github.io/\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "Mining Clues from Incomplete Utterance: A Query-enhanced Network for  Incomplete Utterance Rewriting", "authors": ["Shuzheng Si", "Shuang Zeng", "Baobao Chang"], "url": "https://arxiv.org/abs/2307.00866", "abstract": "Incomplete utterance rewriting has recently raised wide attention. However,\nprevious works do not consider the semantic structural information between\nincomplete utterance and rewritten utterance or model the semantic structure\nimplicitly and insufficiently. To address this problem, we propose a\nQUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly\nbrings guided semantic structural knowledge between the incomplete utterance\nand the rewritten utterance making model perceive where to refer back to or\nrecover omitted tokens. Then, we adopt a fast and effective edit operation\nscoring network to model the relation between two tokens. Benefiting from\nproposed query template and the well-designed edit operation scoring network,\nQUEEN achieves state-of-the-art performance on several public datasets."},
{"title": "Review of Large Vision Models and Visual Prompt Engineering", "authors": ["Jiaqi Wang", "Zhengliang Liu", "Lin Zhao", "Zihao Wu", "Chong Ma", "Sigang Yu", "Haixing Dai", "Qiushi Yang", "Yiheng Liu", "Songyao Zhang", "Enze Shi", "Yi Pan", "Tuo Zhang", "Dajiang Zhu", "Xiang Li", "Xi Jiang", "Bao Ge", "Yixuan Yuan", "Dinggang Shen", "Tianming Liu", "Shu Zhang"], "url": "https://arxiv.org/abs/2307.00855", "abstract": "Visual prompt engineering is a fundamental technology in the field of visual\nand image Artificial General Intelligence, serving as a key component for\nachieving zero-shot capabilities. As the development of large vision models\nprogresses, the importance of prompt engineering becomes increasingly evident.\nDesigning suitable prompts for specific visual tasks has emerged as a\nmeaningful research direction. This review aims to summarize the methods\nemployed in the computer vision domain for large vision models and visual\nprompt engineering, exploring the latest advancements in visual prompt\nengineering. We present influential large models in the visual domain and a\nrange of prompt engineering methods employed on these models. It is our hope\nthat this review provides a comprehensive and systematic description of prompt\nengineering methods based on large visual models, offering valuable insights\nfor future researchers in their exploration of this field."},
{"title": "Augmenting Deep Learning Adaptation for Wearable Sensor Data through  Combined Temporal-Frequency Image Encoding", "authors": ["Yidong Zhu", "Md Mahmudur Rahman", "Mohammad Arif Ul Alam"], "url": "https://arxiv.org/abs/2307.00883", "abstract": "Deep learning advancements have revolutionized scalable classification in\nmany domains including computer vision. However, when it comes to\nwearable-based classification and domain adaptation, existing computer\nvision-based deep learning architectures and pretrained models trained on\nthousands of labeled images for months fall short. This is primarily because\nwearable sensor data necessitates sensor-specific preprocessing, architectural\nmodification, and extensive data collection. To overcome these challenges,\nresearchers have proposed encoding of wearable temporal sensor data in images\nusing recurrent plots. In this paper, we present a novel modified-recurrent\nplot-based image representation that seamlessly integrates both temporal and\nfrequency domain information. Our approach incorporates an efficient Fourier\ntransform-based frequency domain angular difference estimation scheme in\nconjunction with the existing temporal recurrent plot image. Furthermore, we\nemploy mixup image augmentation to enhance the representation. We evaluate the\nproposed method using accelerometer-based activity recognition data and a\npretrained ResNet model, and demonstrate its superior performance compared to\nexisting approaches."},
{"title": "Review helps learn better: Temporal Supervised Knowledge Distillation", "authors": ["Dongwei Wang", "Zhi Han", "Yanmei Wang", "Xiai Chen", "Baichen Liu", "Yandong Tang"], "url": "https://arxiv.org/abs/2307.00811", "abstract": "Reviewing plays an important role when learning knowledge. The knowledge\nacquisition at a certain time point may be strongly inspired with the help of\nprevious experience. Thus the knowledge growing procedure should show strong\nrelationship along the temporal dimension. In our research, we find that during\nthe network training, the evolution of feature map follows temporal sequence\nproperty. A proper temporal supervision may further improve the network\ntraining performance. Inspired by this observation, we design a novel knowledge\ndistillation method. Specifically, we extract the spatiotemporal features in\nthe different training phases of student by convolutional Long Short-term\nmemory network (Conv-LSTM). Then, we train the student net through a dynamic\ntarget, rather than static teacher network features. This process realizes the\nrefinement of old knowledge in student network, and utilizes them to assist\ncurrent learning. Extensive experiments verify the effectiveness and advantages\nof our method over existing knowledge distillation methods, including various\nnetwork architectures, different tasks (image classification and object\ndetection) ."},
{"title": "The ROAD to discovery: machine learning-driven anomaly detection in  radio astronomy spectrograms", "authors": ["Michael Mesarcik", "Albert-Jan Boonstra", "Marco Iacobelli", "Elena Ranguelova", "Cees de Laat", "Rob van Nieuwpoort"], "url": "https://arxiv.org/abs/2307.01054", "abstract": "As radio telescopes increase in sensitivity and flexibility, so do their\ncomplexity and data-rates. For this reason automated system health management\napproaches are becoming increasingly critical to ensure nominal telescope\noperations. We propose a new machine learning anomaly detection framework for\nclassifying both commonly occurring anomalies in radio telescopes as well as\ndetecting unknown rare anomalies that the system has potentially not yet seen.\nTo evaluate our method, we present a dataset consisting of 7050\nautocorrelation-based spectrograms from the Low Frequency Array (LOFAR)\ntelescope and assign 10 different labels relating to the system-wide anomalies\nfrom the perspective of telescope operators. This includes electronic failures,\nmiscalibration, solar storms, network and compute hardware errors among many\nmore. We demonstrate how a novel Self Supervised Learning (SSL) paradigm, that\nutilises both context prediction and reconstruction losses, is effective in\nlearning normal behaviour of the LOFAR telescope. We present the Radio\nObservatory Anomaly Detector (ROAD), a framework that combines both SSL-based\nanomaly detection and a supervised classification, thereby enabling both\nclassification of both commonly occurring anomalies and detection of unseen\nanomalies. We demonstrate that our system is real-time in the context of the\nLOFAR data processing pipeline, requiring &lt;1ms to process a single spectrogram.\nFurthermore, ROAD obtains an anomaly detection F-2 score of 0.92 while\nmaintaining a false positive rate of ~2\\%, as well as a mean per-class\nclassification F-2 score 0.89, outperforming other related works."},
{"title": "Efficient and fully-automatic retinal choroid segmentation in OCT  through DL-based distillation of a hand-crafted pipeline", "authors": ["Jamie Burke", "Justin Engelmann", "Charlene Hamid", "Megan Reid-Schachter", "Tom Pearson", "Dan Pugh", "Neeraj Dhaun", "Stuart King", "Tom MacGillivray", "Miguel O. Bernabeu", "Amos Storkey", "Ian J.C. MacCormick"], "url": "https://arxiv.org/abs/2307.00904", "abstract": "Retinal vascular phenotypes, derived from low-cost, non-invasive retinal\nimaging, have been linked to systemic conditions such as cardio-, neuro- and\nreno-vascular disease. Recent high-resolution optical coherence tomography\n(OCT) allows imaging of the choroidal microvasculature which could provide more\ninformation about vascular health that complements the superficial retinal\nvessels, which current vascular phenotypes are based on. Segmentation of the\nchoroid in OCT is a key step in quantifying choroidal parameters like thickness\nand area. Gaussian Process Edge Tracing (GPET) is a promising, clinically\nvalidated method for this. However, GPET is semi-automatic and thus requires\ntime-consuming manual interventions by specifically trained personnel which\nintroduces subjectivity and limits the potential for analysing larger datasets\nor deploying GPET into clinical practice. We introduce DeepGPET, which distils\nGPET into a neural network to yield a fully-automatic and efficient choroidal\nsegmentation method. DeepGPET achieves excellent agreement with GPET on data\nfrom 3 clinical studies (AUC=0.9994, Dice=0.9664; Pearson correlation of 0.8908\nfor choroidal thickness and 0.9082 for choroidal area), while reducing the mean\nprocessing time per image from 34.49s ($\\pm$15.09) to 1.25s ($\\pm$0.10) on a\nstandard laptop CPU and removing all manual interventions. DeepGPET will be\nmade available for researchers upon publication."},
{"title": "Morse Neural Networks for Uncertainty Quantification", "authors": ["Benoit Dherin", "Huiyi Hu", "Jie Ren", "Michael W. Dusenberry", "Balaji Lakshminarayanan"], "url": "https://arxiv.org/abs/2307.00667", "abstract": "We introduce a new deep generative model useful for uncertainty\nquantification: the Morse neural network, which generalizes the unnormalized\nGaussian densities to have modes of high-dimensional submanifolds instead of\njust discrete points. Fitting the Morse neural network via a KL-divergence loss\nyields 1) a (unnormalized) generative density, 2) an OOD detector, 3) a\ncalibration temperature, 4) a generative sampler, along with in the supervised\ncase 5) a distance aware-classifier. The Morse network can be used on top of a\npre-trained network to bring distance-aware calibration w.r.t the training\ndata. Because of its versatility, the Morse neural networks unifies many\ntechniques: e.g., the Entropic Out-of-Distribution Detector of (Mac\u00eado et\nal., 2021) in OOD detection, the one class Deep Support Vector Description\nmethod of (Ruff et al., 2018) in anomaly detection, or the Contrastive One\nClass classifier in continuous learning (Sun et al., 2021). The Morse neural\nnetwork has connections to support vector machines, kernel methods, and Morse\ntheory in topology."},
{"title": "VoxWatch: An open-set speaker recognition benchmark on VoxCeleb", "authors": ["Raghuveer Peri", "Seyed Omid Sadjadi", "Daniel Garcia-Romero"], "url": "https://arxiv.org/abs/2307.00169", "abstract": "Despite its broad practical applications such as in fraud prevention,\nopen-set speaker identification (OSI) has received less attention in the\nspeaker recognition community compared to speaker verification (SV). OSI deals\nwith determining if a test speech sample belongs to a speaker from a set of\npre-enrolled individuals (in-set) or if it is from an out-of-set speaker. In\naddition to the typical challenges associated with speech variability, OSI is\nprone to the \"false-alarm problem\"; as the size of the in-set speaker\npopulation (a.k.a watchlist) grows, the out-of-set scores become larger,\nleading to increased false alarm rates. This is in particular challenging for\napplications in financial institutions and border security where the watchlist\nsize is typically of the order of several thousand speakers. Therefore, it is\nimportant to systematically quantify the false-alarm problem, and develop\ntechniques that alleviate the impact of watchlist size on detection\nperformance. Prior studies on this problem are sparse, and lack a common\nbenchmark for systematic evaluations. In this paper, we present the first\npublic benchmark for OSI, developed using the VoxCeleb dataset. We quantify the\neffect of the watchlist size and speech duration on the watchlist-based speaker\ndetection task using three strong neural network based systems. In contrast to\nthe findings from prior research, we show that the commonly adopted adaptive\nscore normalization is not guaranteed to improve the performance for this task.\nOn the other hand, we show that score calibration and score fusion, two other\ncommonly used techniques in SV, result in significant improvements in OSI\nperformance."},
{"title": "Why do CNNs excel at feature extraction? A mathematical explanation", "authors": ["Vinoth Nandakumar", "Arush Tagade", "Tongliang Liu"], "url": "https://arxiv.org/abs/2307.00919", "abstract": "Over the past decade deep learning has revolutionized the field of computer\nvision, with convolutional neural network models proving to be very effective\nfor image classification benchmarks. However, a fundamental theoretical\nquestions remain answered: why can they solve discrete image classification\ntasks that involve feature extraction? We address this question in this paper\nby introducing a novel mathematical model for image classification, based on\nfeature extraction, that can be used to generate images resembling real-world\ndatasets. We show that convolutional neural network classifiers can solve these\nimage classification tasks with zero error. In our proof, we construct\npiecewise linear functions that detect the presence of features, and show that\nthey can be realized by a convolutional network."},
{"title": "Uncertainty Informed Optimal Resource Allocation with Gaussian Process  based Bayesian Inference", "authors": ["Samarth Gupta", "Saurabh Amin"], "url": "https://arxiv.org/abs/2307.00032", "abstract": "We focus on the problem of uncertainty informed allocation of medical\nresources (vaccines) to heterogeneous populations for managing epidemic spread.\nWe tackle two related questions: (1) For a compartmental ordinary differential\nequation (ODE) model of epidemic spread, how can we estimate and integrate\nparameter uncertainty into resource allocation decisions? (2) How can we\ncomputationally handle both nonlinear ODE constraints and parameter\nuncertainties for a generic stochastic optimization problem for resource\nallocation? To the best of our knowledge current literature does not fully\nresolve these questions. Here, we develop a data-driven approach to represent\nparameter uncertainty accurately and tractably in a novel stochastic\noptimization problem formulation. We first generate a tractable scenario set by\nestimating the distribution on ODE model parameters using Bayesian inference\nwith Gaussian processes. Next, we develop a parallelized solution algorithm\nthat accounts for scenario-dependent nonlinear ODE constraints. Our\nscenario-set generation procedure and solution approach are flexible in that\nthey can handle any compartmental epidemiological ODE model. Our computational\nexperiments on two different non-linear ODE models (SEIR and SEPIHR) indicate\nthat accounting for uncertainty in key epidemiological parameters can improve\nthe efficacy of time-critical allocation decisions by 4-8%. This improvement\ncan be attributed to data-driven and optimal (strategic) nature of vaccine\nallocations, especially in the early stages of the epidemic when the allocation\nstrategy can crucially impact the long-term trajectory of the disease."},
{"title": "PV Fleet Modeling via Smooth Periodic Gaussian Copula", "authors": ["Mehmet G. Ogut", "Bennet Meyers", "Stephen P. Boyd"], "url": "https://arxiv.org/abs/2307.00004", "abstract": "We present a method for jointly modeling power generation from a fleet of\nphotovoltaic (PV) systems. We propose a white-box method that finds a function\nthat invertibly maps vector time-series data to independent and identically\ndistributed standard normal variables. The proposed method, based on a novel\napproach for fitting a smooth, periodic copula transform to data, captures many\naspects of the data such as diurnal variation in the distribution of power\noutput, dependencies among different PV systems, and dependencies across time.\nIt consists of interpretable steps and is scalable to many systems. The\nresulting joint probability model of PV fleet output across systems and time\ncan be used to generate synthetic data, impute missing data, perform anomaly\ndetection, and make forecasts. In this paper, we explain the method and\ndemonstrate these applications."},
{"title": "NeuBTF: Neural fields for BTF encoding and transfer", "authors": ["Carlos Rodriguez-Pardo", "Konstantinos Kazatzis", "Jorge Lopez-Moreno", "Elena Garces"], "url": "https://arxiv.org/abs/2307.01199", "abstract": "Neural material representations are becoming a popular way to represent\nmaterials for rendering. They are more expressive than analytic models and\noccupy less memory than tabulated BTFs. However, existing neural materials are\nimmutable, meaning that their output for a certain query of UVs, camera, and\nlight vector is fixed once they are trained. While this is practical when there\nis no need to edit the material, it can become very limiting when the fragment\nof the material used for training is too small or not tileable, which\nfrequently happens when the material has been captured with a\ngonioreflectometer. In this paper, we propose a novel neural material\nrepresentation which jointly tackles the problems of BTF compression, tiling,\nand extrapolation. At test time, our method uses a guidance image as input to\ncondition the neural BTF to the structural features of this input image. Then,\nthe neural BTF can be queried as a regular BTF using UVs, camera, and light\nvectors. Every component in our framework is purposefully designed to maximize\nBTF encoding quality at minimal parameter count and computational complexity,\nachieving competitive compression rates compared with previous work. We\ndemonstrate the results of our method on a variety of synthetic and captured\nmaterials, showing its generality and capacity to learn to represent many\noptical properties."},
{"title": "Squeezing Large-Scale Diffusion Models for Mobile", "authors": ["Jiwoong Choi", "Minkyu Kim", "Daehyun Ahn", "Taesu Kim", "Yulhwa Kim", "Dongwon Jo", "Hyesung Jeon", "Jae-Joon Kim", "Hyungjun Kim"], "url": "https://arxiv.org/abs/2307.01193", "abstract": "The emergence of diffusion models has greatly broadened the scope of\nhigh-fidelity image synthesis, resulting in notable advancements in both\npractical implementation and academic research. With the active adoption of the\nmodel in various real-world applications, the need for on-device deployment has\ngrown considerably. However, deploying large diffusion models such as Stable\nDiffusion with more than one billion parameters to mobile devices poses\ndistinctive challenges due to the limited computational and memory resources,\nwhich may vary according to the device. In this paper, we present the\nchallenges and solutions for deploying Stable Diffusion on mobile devices with\nTensorFlow Lite framework, which supports both iOS and Android devices. The\nresulting Mobile Stable Diffusion achieves the inference latency of smaller\nthan 7 seconds for a 512x512 image generation on Android devices with mobile\nGPUs."},
{"title": "SAMAug: Point Prompt Augmentation for Segment Anything Model", "authors": ["Haixing Dai", "Chong Ma", "Zhengliang Liu", "Yiwei Li", "Peng Shu", "Xiaozheng Wei", "Lin Zhao", "Zihao Wu", "Dajiang Zhu", "Wei Liu", "Quanzheng Li", "Tianming Liu", "Xiang Li"], "url": "https://arxiv.org/abs/2307.01187", "abstract": "This paper introduces SAMAug, a novel visual point augmentation method for\nthe Segment Anything Model (SAM) that enhances interactive image segmentation\nperformance. SAMAug generates augmented point prompts to provide more\ninformation to SAM. From the initial point prompt, SAM produces the initial\nmask, which is then fed into our proposed SAMAug to generate augmented point\nprompts. By incorporating these extra points, SAM can generate augmented\nsegmentation masks based on the augmented point prompts and the initial prompt,\nresulting in improved segmentation performance. We evaluate four point\naugmentation techniques: random selection, maximum difference entropy, maximum\ndistance, and a saliency model. Experiments on the COCO, Fundus, and Chest\nX-ray datasets demonstrate that SAMAug can boost SAM's segmentation results,\nespecially using the maximum distance and saliency model methods. SAMAug\nunderscores the potential of visual prompt engineering to advance interactive\ncomputer vision models."},
{"title": "Don't freeze: Finetune encoders for better Self-Supervised HAR", "authors": ["Vitor Fortes Rey", "Dominique Nshimyimana", "Paul Lukowicz"], "url": "https://arxiv.org/abs/2307.01168", "abstract": "Recently self-supervised learning has been proposed in the field of human\nactivity recognition as a solution to the labelled data availability problem.\nThe idea being that by using pretext tasks such as reconstruction or\ncontrastive predictive coding, useful representations can be learned that then\ncan be used for classification. Those approaches follow the pretrain, freeze\nand fine-tune procedure. In this paper we will show how a simple change - not\nfreezing the representation - leads to substantial performance gains across\npretext tasks. The improvement was found in all four investigated datasets and\nacross all four pretext tasks and is inversely proportional to amount of\nlabelled data. Moreover the effect is present whether the pretext task is\ncarried on the Capture24 dataset or directly in unlabelled data of the target\ndataset."},
{"title": "Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement  Learning", "authors": ["Ini Oguntola", "Joseph Campbell", "Simon Stepputtis", "Katia Sycara"], "url": "https://arxiv.org/abs/2307.01158", "abstract": "The ability to model the mental states of others is crucial to human social\nintelligence, and can offer similar benefits to artificial agents with respect\nto the social dynamics induced in multi-agent settings. We present a method of\ngrounding semantically meaningful, human-interpretable beliefs within policies\nmodeled by deep networks. We then consider the task of 2nd-order belief\nprediction. We propose that ability of each agent to predict the beliefs of the\nother agents can be used as an intrinsic reward signal for multi-agent\nreinforcement learning. Finally, we present preliminary empirical results in a\nmixed cooperative-competitive environment."},
{"title": "Soft Gripping: Specifying for Trustworthiness", "authors": ["Dhaminda B. Abeywickrama", "Nguyen Hao Le", "Greg Chance", "Peter D. Winter", "Arianna Manzini", "Alix J. Partridge", "Jonathan Ives", "John Downer", "Graham Deacon", "Jonathan Rossiter", "Kerstin Eder", "Shane Windsor"], "url": "https://arxiv.org/abs/2307.01159", "abstract": "Soft robotics is an emerging technology in which engineers create flexible\ndevices for use in a variety of applications. In order to advance the wide\nadoption of soft robots, ensuring their trustworthiness is essential; if soft\nrobots are not trusted, they will not be used to their full potential. In order\nto demonstrate trustworthiness, a specification needs to be formulated to\ndefine what is trustworthy. However, even for soft robotic grippers, which is\none of the most mature areas in soft robotics, the soft robotics community has\nso far given very little attention to formulating specifications. In this work,\nwe discuss the importance of developing specifications during development of\nsoft robotic systems, and present an extensive example specification for a soft\ngripper for pick-and-place tasks for grocery items. The proposed specification\ncovers both functional and non-functional requirements, such as reliability,\nsafety, adaptability, predictability, ethics, and regulations. We also\nhighlight the need to promote verifiability as a first-class objective in the\ndesign of a soft gripper."},
{"title": "PlanE: Representation Learning over Planar Graphs", "authors": ["Radoslav Dimitrov", "Zeyang Zhao", "Ralph Abboud", "\u0130smail \u0130lkan Ceylan"], "url": "https://arxiv.org/abs/2307.01180", "abstract": "Graph neural networks are prominent models for representation learning over\ngraphs, where the idea is to iteratively compute representations of nodes of an\ninput graph through a series of transformations in such a way that the learned\ngraph function is isomorphism invariant on graphs, which makes the learned\nrepresentations graph invariants. On the other hand, it is well-known that\ngraph invariants learned by these class of models are incomplete: there are\npairs of non-isomorphic graphs which cannot be distinguished by standard graph\nneural networks. This is unsurprising given the computational difficulty of\ngraph isomorphism testing on general graphs, but the situation begs to differ\nfor special graph classes, for which efficient graph isomorphism testing\nalgorithms are known, such as planar graphs. The goal of this work is to design\narchitectures for efficiently learning complete invariants of planar graphs.\nInspired by the classical planar graph isomorphism algorithm of Hopcroft and\nTarjan, we propose PlanE as a framework for planar representation learning.\nPlanE includes architectures which can learn complete invariants over planar\ngraphs while remaining practically scalable. We empirically validate the strong\nperformance of the resulting model architectures on well-known planar graph\nbenchmarks, achieving multiple state-of-the-art results."},
{"title": "Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction", "authors": ["Salvatore Carta", "Alessandro Giuliani", "Leonardo Piano", "Alessandro Sebastian Podda", "Livio Pompianu", "Sandro Gabriele Tiddia"], "url": "https://arxiv.org/abs/2307.01128", "abstract": "In the current digitalization era, capturing and effectively representing\nknowledge is crucial in most real-world scenarios. In this context, knowledge\ngraphs represent a potent tool for retrieving and organizing a vast amount of\ninformation in a properly interconnected and interpretable structure. However,\ntheir generation is still challenging and often requires considerable human\neffort and domain expertise, hampering the scalability and flexibility across\ndifferent application fields. This paper proposes an innovative knowledge graph\ngeneration approach that leverages the potential of the latest generative large\nlanguage models, such as GPT-3.5, that can address all the main critical issues\nin knowledge graph building. The approach is conveyed in a pipeline that\ncomprises novel iterative zero-shot and external knowledge-agnostic strategies\nin the main stages of the generation process. Our unique manifold approach may\nencompass significant benefits to the scientific community. In particular, the\nmain contribution can be summarized by: (i) an innovative strategy for\niteratively prompting large language models to extract relevant components of\nthe final graph; (ii) a zero-shot strategy for each prompt, meaning that there\nis no need for providing examples for \"guiding\" the prompt result; (iii) a\nscalable solution, as the adoption of LLMs avoids the need for any external\nresources or human expertise. To assess the effectiveness of our proposed\nmodel, we performed experiments on a dataset that covered a specific domain. We\nclaim that our proposal is a suitable solution for scalable and versatile\nknowledge graph construction and may be applied to different and novel\ncontexts."},
{"title": "SCITUNE: Aligning Large Language Models with Scientific Multimodal  Instructions", "authors": ["Sameera Horawalavithana", "Sai Munikoti", "Ian Stewart", "Henry Kvinge"], "url": "https://arxiv.org/abs/2307.01139", "abstract": "Instruction finetuning is a popular paradigm to align large language models\n(LLM) with human intent. Despite its popularity, this idea is less explored in\nimproving the LLMs to align existing foundation models with scientific\ndisciplines, concepts and goals. In this work, we present SciTune as a tuning\nframework to improve the ability of LLMs to follow scientific multimodal\ninstructions. To test our methodology, we use a human-generated scientific\ninstruction tuning dataset and train a large multimodal model LLaMA-SciTune\nthat connects a vision encoder and LLM for science-focused visual and language\nunderstanding. In comparison to the models that are finetuned with machine\ngenerated data only, LLaMA-SciTune surpasses human performance on average and\nin many sub-categories on the ScienceQA benchmark."},
{"title": "Some challenges of calibrating differentiable agent-based models", "authors": ["Arnau Quera-Bofarull", "Joel Dyer", "Anisoara Calinescu", "Michael Wooldridge"], "url": "https://arxiv.org/abs/2307.01085", "abstract": "Agent-based models (ABMs) are a promising approach to modelling and reasoning\nabout complex systems, yet their application in practice is impeded by their\ncomplexity, discrete nature, and the difficulty of performing parameter\ninference and optimisation tasks. This in turn has sparked interest in the\nconstruction of differentiable ABMs as a strategy for combatting these\ndifficulties, yet a number of challenges remain. In this paper, we discuss and\npresent experiments that highlight some of these challenges, along with\npotential solutions."},
{"title": "Exploring the In-context Learning Ability of Large Language Model for  Biomedical Concept Linking", "authors": ["Qinyong Wang", "Zhenxiang Gao", "Rong Xu"], "url": "https://arxiv.org/abs/2307.01137", "abstract": "The biomedical field relies heavily on concept linking in various areas such\nas literature mining, graph alignment, information retrieval,\nquestion-answering, data, and knowledge integration. Although large language\nmodels (LLMs) have made significant strides in many natural language processing\ntasks, their effectiveness in biomedical concept mapping is yet to be fully\nexplored. This research investigates a method that exploits the in-context\nlearning (ICL) capabilities of large models for biomedical concept linking. The\nproposed approach adopts a two-stage retrieve-and-rank framework. Initially,\nbiomedical concepts are embedded using language models, and then embedding\nsimilarity is utilized to retrieve the top candidates. These candidates'\ncontextual information is subsequently incorporated into the prompt and\nprocessed by a large language model to re-rank the concepts. This approach\nachieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7%\nin chemical entity normalization, exhibiting a competitive performance relative\nto supervised learning methods. Further, it showed a significant improvement,\nwith an over 20-point absolute increase in F1 score on an oncology matching\ndataset. Extensive qualitative assessments were conducted, and the benefits and\npotential shortcomings of using large language models within the biomedical\ndomain were discussed. were discussed."},
{"title": "ENGAGE: Explanation Guided Data Augmentation for Graph Representation  Learning", "authors": ["Yucheng Shi", "Kaixiong Zhou", "Ninghao Liu"], "url": "https://arxiv.org/abs/2307.01053", "abstract": "The recent contrastive learning methods, due to their effectiveness in\nrepresentation learning, have been widely applied to modeling graph data.\nRandom perturbation is widely used to build contrastive views for graph data,\nwhich however, could accidentally break graph structures and lead to suboptimal\nperformance. In addition, graph data is usually highly abstract, so it is hard\nto extract intuitive meanings and design more informed augmentation schemes.\nEffective representations should preserve key characteristics in data and\nabandon superfluous information. In this paper, we propose ENGAGE (ExplaNation\nGuided data AuGmEntation), where explanation guides the contrastive\naugmentation process to preserve the key parts in graphs and explore removing\nsuperfluous information. Specifically, we design an efficient unsupervised\nexplanation method called smoothed activation map as the indicator of node\nimportance in representation learning. Then, we design two data augmentation\nschemes on graphs for perturbing structural and feature information,\nrespectively. We also provide justification for the proposed method in the\nframework of information theories. Experiments of both graph-level and\nnode-level tasks, on various model architectures and on different real-world\ngraphs, are conducted to demonstrate the effectiveness and flexibility of\nENGAGE. The code of ENGAGE can be found: <a class=\"link-external link-https\" href=\"https://github.com/sycny/ENGAGE\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "Temporal Graph Benchmark for Machine Learning on Temporal Graphs", "authors": ["Shenyang Huang", "Farimah Poursafaei", "Jacob Danovitch", "Matthias Fey", "Weihua Hu", "Emanuele Rossi", "Jure Leskovec", "Michael Bronstein", "Guillaume Rabusseau", "Reihaneh Rabbany"], "url": "https://arxiv.org/abs/2307.01026", "abstract": "We present the Temporal Graph Benchmark (TGB), a collection of challenging\nand diverse benchmark datasets for realistic, reproducible, and robust\nevaluation of machine learning models on temporal graphs. TGB datasets are of\nlarge scale, spanning years in duration, incorporate both node and edge-level\nprediction tasks and cover a diverse set of domains including social, trade,\ntransaction, and transportation networks. For both tasks, we design evaluation\nprotocols based on realistic use-cases. We extensively benchmark each dataset\nand find that the performance of common models can vary drastically across\ndatasets. In addition, on dynamic node property prediction tasks, we show that\nsimple methods often achieve superior performance compared to existing temporal\ngraph models. We believe that these findings open up opportunities for future\nresearch on temporal graphs. Finally, TGB provides an automated machine\nlearning pipeline for reproducible and accessible temporal graph research,\nincluding data loading, experiment setup and performance evaluation. TGB will\nbe maintained and updated on a regular basis and welcomes community feedback.\nTGB datasets, data loaders, example codes, evaluation setup, and leaderboards\nare publicly available at <a class=\"link-external link-https\" href=\"https://tgb.complexdatalab.com/\" rel=\"external noopener nofollow\">this https URL</a> ."},
{"title": "RefSAM: Efficiently Adapting Segmenting Anything Model for Referring  Video Object Segmentation", "authors": ["Yonglin Li", "Jing Zhang", "Xiao Teng", "Long Lan"], "url": "https://arxiv.org/abs/2307.00997", "abstract": "The Segment Anything Model (SAM) has gained significant attention for its\nimpressive performance in image segmentation. However, it lacks proficiency in\nreferring video object segmentation (RVOS) due to the need for precise\nuser-interactive prompts and limited understanding of different modalities,\nsuch as language and vision. This paper presents the RefSAM model, which for\nthe first time explores the potential of SAM for RVOS by incorporating\nmulti-view information from diverse modalities and successive frames at\ndifferent timestamps. Our proposed approach adapts the original SAM model to\nenhance cross-modality learning by employing a lightweight Cross-Modal MLP that\nprojects the text embedding of the referring expression into sparse and dense\nembeddings, serving as user-interactive prompts. Subsequently, a\nparameter-efficient tuning strategy is employed to effectively align and fuse\nthe language and vision features. Through comprehensive ablation studies, we\ndemonstrate the practical and effective design choices of our strategy.\nExtensive experiments conducted on Ref-Youtu-VOS and Ref-DAVIS17 datasets\nvalidate the superiority and effectiveness of our RefSAM model over existing\nmethods. The code and models will be made publicly at\n\\href{<a class=\"link-external link-https\" href=\"https://github.com/LancasterLi/RefSAM\" rel=\"external noopener nofollow\">this https URL</a>}{<a class=\"link-external link-http\" href=\"http://github.com/LancasterLi/RefSAM\" rel=\"external noopener nofollow\">this http URL</a>}."},
{"title": "REAL: A Representative Error-Driven Approach for Active Learning", "authors": ["Cheng Chen", "Yong Wang", "Lizi Liao", "Yueguo Chen", "Xiaoyong Du"], "url": "https://arxiv.org/abs/2307.00968", "abstract": "Given a limited labeling budget, active learning (AL) aims to sample the most\ninformative instances from an unlabeled pool to acquire labels for subsequent\nmodel training. To achieve this, AL typically measures the informativeness of\nunlabeled instances based on uncertainty and diversity. However, it does not\nconsider erroneous instances with their neighborhood error density, which have\ngreat potential to improve the model performance. To address this limitation,\nwe propose $REAL$, a novel approach to select data instances with\n$\\underline{R}$epresentative $\\underline{E}$rrors for $\\underline{A}$ctive\n$\\underline{L}$earning. It identifies minority predictions as \\emph{pseudo\nerrors} within a cluster and allocates an adaptive sampling budget for the\ncluster based on estimated error density. Extensive experiments on five text\nclassification datasets demonstrate that $REAL$ consistently outperforms all\nbest-performing baselines regarding accuracy and F1-macro scores across a wide\nrange of hyperparameter settings. Our analysis also shows that $REAL$ selects\nthe most representative pseudo errors that match the distribution of\nground-truth errors along the decision boundary. Our code is publicly available\nat <a class=\"link-external link-https\" href=\"https://github.com/withchencheng/ECML_PKDD_23_Real\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "OpenClinicalAI: An Open and Dynamic Model for Alzheimer's Disease  Diagnosis", "authors": ["Yunyou Huang", "Xiaoshuang Liang", "Xiangjiang Lu", "Xiuxia Miao", "Jiyue Xie", "Wenjing Liu", "Fan Zhang", "Guoxin Kang", "Li Ma", "Suqin Tang", "Zhifei Zhang", "Jianfeng Zhan"], "url": "https://arxiv.org/abs/2307.00965", "abstract": "Although Alzheimer's disease (AD) cannot be reversed or cured, timely\ndiagnosis can significantly reduce the burden of treatment and care. Current\nresearch on AD diagnosis models usually regards the diagnosis task as a typical\nclassification task with two primary assumptions: 1) All target categories are\nknown a priori; 2) The diagnostic strategy for each patient is consistent, that\nis, the number and type of model input data for each patient are the same.\nHowever, real-world clinical settings are open, with complexity and uncertainty\nin terms of both subjects and the resources of the medical institutions. This\nmeans that diagnostic models may encounter unseen disease categories and need\nto dynamically develop diagnostic strategies based on the subject's specific\ncircumstances and available medical resources. Thus, the AD diagnosis task is\ntangled and coupled with the diagnosis strategy formulation. To promote the\napplication of diagnostic systems in real-world clinical settings, we propose\nOpenClinicalAI for direct AD diagnosis in complex and uncertain clinical\nsettings. This is the first powerful end-to-end model to dynamically formulate\ndiagnostic strategies and provide diagnostic results based on the subject's\nconditions and available medical resources. OpenClinicalAI combines\nreciprocally coupled deep multiaction reinforcement learning (DMARL) for\ndiagnostic strategy formulation and multicenter meta-learning (MCML) for\nopen-set recognition. The experimental results show that OpenClinicalAI\nachieves better performance and fewer clinical examinations than the\nstate-of-the-art model. Our method provides an opportunity to embed the AD\ndiagnostic system into the current health care system to cooperate with\nclinicians to improve current health care."},
{"title": "Evaluating Shutdown Avoidance of Language Models in Textual Scenarios", "authors": ["Teun van der Weij", "Simon Lermen", "Leon lang"], "url": "https://arxiv.org/abs/2307.00787", "abstract": "Recently, there has been an increase in interest in evaluating large language\nmodels for emergent and dangerous capabilities. Importantly, agents could\nreason that in some scenarios their goal is better achieved if they are not\nturned off, which can lead to undesirable behaviors. In this paper, we\ninvestigate the potential of using toy textual scenarios to evaluate\ninstrumental reasoning and shutdown avoidance in language models such as GPT-4\nand Claude. Furthermore, we explore whether shutdown avoidance is merely a\nresult of simple pattern matching between the dataset and the prompt or if it\nis a consistent behaviour across different environments and variations.\n<br>We evaluated behaviours manually and also experimented with using language\nmodels for automatic evaluations, and these evaluations demonstrate that simple\npattern matching is likely not the sole contributing factor for shutdown\navoidance. This study provides insights into the behaviour of language models\nin shutdown avoidance scenarios and inspires further research on the use of\ntextual scenarios for evaluations."},
{"title": "DifFSS: Diffusion Model for Few-Shot Semantic Segmentation", "authors": ["Weimin Tan", "Siyuan Chen", "Bo Yan"], "url": "https://arxiv.org/abs/2307.00773", "abstract": "Diffusion models have demonstrated excellent performance in image generation.\nAlthough various few-shot semantic segmentation (FSS) models with different\nnetwork structures have been proposed, performance improvement has reached a\nbottleneck. This paper presents the first work to leverage the diffusion model\nfor FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve\nthe performance of the state-of-the-art FSS models by a large margin without\nmodifying their network structure. Specifically, we utilize the powerful\ngeneration ability of diffusion models to generate diverse auxiliary support\nimages by using the semantic mask, scribble or soft HED boundary of the support\nimage as control conditions. This generation process simulates the variety\nwithin the class of the query image, such as color, texture variation,\nlighting, $etc$. As a result, FSS models can refer to more diverse support\nimages, yielding more robust representations, thereby achieving a consistent\nimprovement in segmentation performance. Extensive experiments on three\npublicly available datasets based on existing advanced FSS models demonstrate\nthe effectiveness of the diffusion model for FSS task. Furthermore, we explore\nin detail the impact of different input settings of the diffusion model on\nsegmentation performance. Hopefully, this completely new paradigm will bring\ninspiration to the study of FSS task integrated with AI-generated content."},
{"title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph  Reading", "authors": ["Yujia Xiao", "Shaofei Zhang", "Xi Wang", "Xu Tan", "Lei He", "Sheng Zhao", "Frank K. Soong", "Tan Lee"], "url": "https://arxiv.org/abs/2307.00782", "abstract": "While state-of-the-art Text-to-Speech systems can generate natural speech of\nvery high quality at sentence level, they still meet great challenges in speech\ngeneration for paragraph / long-form reading. Such deficiencies are due to i)\nignorance of cross-sentence contextual information, and ii) high computation\nand memory cost for long-form synthesis. To address these issues, this work\ndevelops a lightweight yet effective TTS system, ContextSpeech. Specifically,\nwe first design a memory-cached recurrence mechanism to incorporate global text\nand speech context into sentence encoding. Then we construct\nhierarchically-structured textual semantics to broaden the scope for global\ncontext enhancement. Additionally, we integrate linearized self-attention to\nimprove model efficiency. Experiments show that ContextSpeech significantly\nimproves the voice quality and prosody expressiveness in paragraph reading with\ncompetitive model efficiency. Audio samples are available at:\n<a class=\"link-external link-https\" href=\"https://contextspeech.github.io/demo/\" rel=\"external noopener nofollow\">this https URL</a>"},
{"title": "Hierarchical Open-vocabulary Universal Image Segmentation", "authors": ["Xudong Wang", "Shufan Li", "Konstantinos Kallidromitis", "Yusuke Kato", "Kazuki Kozuka", "Trevor Darrell"], "url": "https://arxiv.org/abs/2307.00764", "abstract": "Open-vocabulary image segmentation aims to partition an image into semantic\nregions according to arbitrary text descriptions. However, complex visual\nscenes can be naturally decomposed into simpler parts and abstracted at\nmultiple levels of granularity, introducing inherent segmentation ambiguity.\nUnlike existing methods that typically sidestep this ambiguity and treat it as\nan external factor, our approach actively incorporates a hierarchical\nrepresentation encompassing different semantic-levels into the learning\nprocess. We propose a decoupled text-image fusion mechanism and representation\nlearning modules for both \"things\" and \"stuff\".1 Additionally, we\nsystematically examine the differences that exist in the textual and visual\nfeatures between these types of categories. Our resulting model, named HIPIE,\ntackles HIerarchical, oPen-vocabulary, and unIvErsal segmentation tasks within\na unified framework. Benchmarked on over 40 datasets, e.g., ADE20K, COCO,\nPascal-VOC Part, RefCOCO/RefCOCOg, ODinW and SeginW, HIPIE achieves the\nstate-of-the-art results at various levels of image comprehension, including\nsemantic-level (e.g., semantic segmentation), instance-level (e.g.,\npanoptic/referring segmentation and object detection), as well as part-level\n(e.g., part/subpart segmentation) tasks. Our code is released at\n<a class=\"link-external link-https\" href=\"https://github.com/berkeley-hipie/HIPIE\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "Feasibility of Universal Anomaly Detection without Knowing the  Abnormality in Medical Images", "authors": ["Can Cui", "Yaohong Wang", "Shunxing Bao", "Yucheng Tang", "Ruining Deng", "Lucas W. Remedios", "Zuhayr Asad", "Joseph T. Roland", "Ken S. Lau", "Qi Liu", "Lori A. Coburn", "Keith T. Wilson", "Bennett A. Landman", "Yuankai Huo"], "url": "https://arxiv.org/abs/2307.00750", "abstract": "Many anomaly detection approaches, especially deep learning methods, have\nbeen recently developed to identify abnormal image morphology by only employing\nnormal images during training. Unfortunately, many prior anomaly detection\nmethods were optimized for a specific \"known\" abnormality (e.g., brain tumor,\nbone fraction, cell types). Moreover, even though only the normal images were\nused in the training process, the abnormal images were oftenly employed during\nthe validation process (e.g., epoch selection, hyper-parameter tuning), which\nmight leak the supposed ``unknown\" abnormality unintentionally. In this study,\nwe investigated these two essential aspects regarding universal anomaly\ndetection in medical images by (1) comparing various anomaly detection methods\nacross four medical datasets, (2) investigating the inevitable but often\nneglected issues on how to unbiasedly select the optimal anomaly detection\nmodel during the validation phase using only normal images, and (3) proposing a\nsimple decision-level ensemble method to leverage the advantage of different\nkinds of anomaly detection without knowing the abnormality. The results of our\nexperiments indicate that none of the evaluated methods consistently achieved\nthe best performance across all datasets. Our proposed method enhanced the\nrobustness of performance in general (average AUC 0.956)."},
{"title": "Population Age Group Sensitivity for COVID-19 Infections with Deep  Learning", "authors": ["Md Khairul Islam", "Tyler Valentine", "Royal Wang", "Levi Davis", "Matt Manner", "Judy Fox"], "url": "https://arxiv.org/abs/2307.00751", "abstract": "The COVID-19 pandemic has created unprecedented challenges for governments\nand healthcare systems worldwide, highlighting the critical importance of\nunderstanding the factors that contribute to virus transmission. This study\naimed to identify the most influential age groups in COVID-19 infection rates\nat the US county level using the Modified Morris Method and deep learning for\ntime series. Our approach involved training the state-of-the-art time-series\nmodel Temporal Fusion Transformer on different age groups as a static feature\nand the population vaccination status as the dynamic feature. We analyzed the\nimpact of those age groups on COVID-19 infection rates by perturbing individual\ninput features and ranked them based on their Morris sensitivity scores, which\nquantify their contribution to COVID-19 transmission rates. The findings are\nverified using ground truth data from the CDC and US Census, which provide the\ntrue infection rates for each age group. The results suggest that young adults\nwere the most influential age group in COVID-19 transmission at the county\nlevel between March 1, 2020, and November 27, 2021. Using these results can\ninform public health policies and interventions, such as targeted vaccination\nstrategies, to better control the spread of the virus. Our approach\ndemonstrates the utility of feature sensitivity analysis in identifying\ncritical factors contributing to COVID-19 transmission and can be applied in\nother public health domains."},
{"title": "GA-DRL: Graph Neural Network-Augmented Deep Reinforcement Learning for  DAG Task Scheduling over Dynamic Vehicular Clouds", "authors": ["Zhang Liu", "Lianfen Huang", "Zhibin Gao", "Manman Luo", "Seyyedali Hosseinalipour", "Huaiyu Dai"], "url": "https://arxiv.org/abs/2307.00777", "abstract": "Vehicular clouds (VCs) are modern platforms for processing of\ncomputation-intensive tasks over vehicles. Such tasks are often represented as\ndirected acyclic graphs (DAGs) consisting of interdependent vertices/subtasks\nand directed edges. In this paper, we propose a graph neural network-augmented\ndeep reinforcement learning scheme (GA-DRL) for scheduling DAG tasks over\ndynamic VCs. In doing so, we first model the VC-assisted DAG task scheduling as\na Markov decision process. We then adopt a multi-head graph attention network\n(GAT) to extract the features of DAG subtasks. Our developed GAT enables a\ntwo-way aggregation of the topological information in a DAG task by\nsimultaneously considering predecessors and successors of each subtask. We\nfurther introduce non-uniform DAG neighborhood sampling through codifying the\nscheduling priority of different subtasks, which makes our developed GAT\ngeneralizable to completely unseen DAG task topologies. Finally, we augment GAT\ninto a double deep Q-network learning module to conduct subtask-to-vehicle\nassignment according to the extracted features of subtasks, while considering\nthe dynamics and heterogeneity of the vehicles in VCs. Through simulating\nvarious DAG tasks under real-world movement traces of vehicles, we demonstrate\nthat GA-DRL outperforms existing benchmarks in terms of DAG task completion\ntime."},
{"title": "DSTCGCN: Learning Dynamic Spatial-Temporal Cross Dependencies for  Traffic Forecasting", "authors": ["Binqing Wu", "Ling Chen"], "url": "https://arxiv.org/abs/2307.00518", "abstract": "Traffic forecasting is essential to intelligent transportation systems, which\nis challenging due to the complicated spatial and temporal dependencies within\na road network. Existing works usually learn spatial and temporal dependencies\nseparately, ignoring the dependencies crossing spatial and temporal dimensions.\nIn this paper, we propose DSTCGCN, a dynamic spatial-temporal cross graph\nconvolution network to learn dynamic spatial and temporal dependencies jointly\nvia graphs for traffic forecasting. Specifically, we introduce a fast Fourier\ntransform (FFT) based attentive selector to choose relevant time steps for each\ntime step based on time-varying traffic data. Given the selected time steps, we\nintroduce a dynamic cross graph construction module, consisting of the spatial\ngraph construction, temporal connection graph construction, and fusion modules,\nto learn dynamic spatial-temporal cross dependencies without pre-defined\npriors. Extensive experiments on six real-world datasets demonstrate that\nDSTCGCN achieves the state-of-the-art performance."},
{"title": "HeGeL: A Novel Dataset for Geo-Location from Hebrew Text", "authors": ["Tzuf Paz-Argaman", "Tal Bauman", "Itai Mondshine", "Itzhak Omer", "Sagi Dalyot", "Reut Tsarfaty"], "url": "https://arxiv.org/abs/2307.00509", "abstract": "The task of textual geolocation - retrieving the coordinates of a place based\non a free-form language description - calls for not only grounding but also\nnatural language understanding and geospatial reasoning. Even though there are\nquite a few datasets in English used for geolocation, they are currently based\non open-source data (Wikipedia and Twitter), where the location of the\ndescribed place is mostly implicit, such that the location retrieval resolution\nis limited. Furthermore, there are no datasets available for addressing the\nproblem of textual geolocation in morphologically rich and resource-poor\nlanguages, such as Hebrew. In this paper, we present the Hebrew Geo-Location\n(HeGeL) corpus, designed to collect literal place descriptions and analyze\nlingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place\ndescriptions of various place types in three cities in Israel. Qualitative and\nempirical analysis show that the data exhibits abundant use of geospatial\nreasoning and requires a novel environmental representation."},
{"title": "On efficient computation in active inference", "authors": ["Aswin Paul", "Noor Sajid", "Lancelot Da Costa", "Adeel Razi"], "url": "https://arxiv.org/abs/2307.00504", "abstract": "Despite being recognized as neurobiologically plausible, active inference\nfaces difficulties when employed to simulate intelligent behaviour in complex\nenvironments due to its computational cost and the difficulty of specifying an\nappropriate target distribution for the agent. This paper introduces two\nsolutions that work in concert to address these limitations. First, we present\na novel planning algorithm for finite temporal horizons with drastically lower\ncomputational complexity. Second, inspired by Z-learning from control theory\nliterature, we simplify the process of setting an appropriate target\ndistribution for new and existing active inference planning schemes. Our first\napproach leverages the dynamic programming algorithm, known for its\ncomputational efficiency, to minimize the cost function used in planning\nthrough the Bellman-optimality principle. Accordingly, our algorithm\nrecursively assesses the expected free energy of actions in the reverse\ntemporal order. This improves computational efficiency by orders of magnitude\nand allows precise model learning and planning, even under uncertain\nconditions. Our method simplifies the planning process and shows meaningful\nbehaviour even when specifying only the agent's final goal state. The proposed\nsolutions make defining a target distribution from a goal state straightforward\ncompared to the more complicated task of defining a temporally informed target\ndistribution. The effectiveness of these methods is tested and demonstrated\nthrough simulations in standard grid-world tasks. These advances create new\nopportunities for various applications."},
{"title": "Don't Memorize; Mimic The Past: Federated Class Incremental Learning  Without Episodic Memory", "authors": ["Sara Babakniya", "Zalan Fabian", "Chaoyang He", "Mahdi Soltanolkotabi", "Salman Avestimehr"], "url": "https://arxiv.org/abs/2307.00497", "abstract": "Deep learning models are prone to forgetting information learned in the past\nwhen trained on new data. This problem becomes even more pronounced in the\ncontext of federated learning (FL), where data is decentralized and subject to\nindependent changes for each user. Continual Learning (CL) studies this\nso-called \\textit{catastrophic forgetting} phenomenon primarily in centralized\nsettings, where the learner has direct access to the complete training dataset.\nHowever, applying CL techniques to FL is not straightforward due to privacy\nconcerns and resource limitations. This paper presents a framework for\nfederated class incremental learning that utilizes a generative model to\nsynthesize samples from past distributions instead of storing part of past\ndata. Then, clients can leverage the generative model to mitigate catastrophic\nforgetting locally. The generative model is trained on the server using\ndata-free methods at the end of each task without requesting data from clients.\nTherefore, it reduces the risk of data leakage as opposed to training it on the\nclient's private data. We demonstrate significant improvements for the\nCIFAR-100 dataset compared to existing baselines."},
{"title": "STG4Traffic: A Survey and Benchmark of Spatial-Temporal Graph Neural  Networks for Traffic Prediction", "authors": ["Xunlian Luo", "Chunjiang Zhu", "Detian Zhang", "Qing Li"], "url": "https://arxiv.org/abs/2307.00495", "abstract": "Traffic prediction has been an active research topic in the domain of\nspatial-temporal data mining. Accurate real-time traffic prediction is\nessential to improve the safety, stability, and versatility of smart city\nsystems, i.e., traffic control and optimal routing. The complex and highly\ndynamic spatial-temporal dependencies make effective predictions still face\nmany challenges. Recent studies have shown that spatial-temporal graph neural\nnetworks exhibit great potential applied to traffic prediction, which combines\nsequential models with graph convolutional networks to jointly model temporal\nand spatial correlations. However, a survey study of graph learning,\nspatial-temporal graph models for traffic, as well as a fair comparison of\nbaseline models are pending and unavoidable issues. In this paper, we first\nprovide a systematic review of graph learning strategies and commonly used\ngraph convolution algorithms. Then we conduct a comprehensive analysis of the\nstrengths and weaknesses of recently proposed spatial-temporal graph network\nmodels. Furthermore, we build a study called STG4Traffic using the deep\nlearning framework PyTorch to establish a standardized and scalable benchmark\non two types of traffic datasets. We can evaluate their performance by\npersonalizing the model settings with uniform metrics. Finally, we point out\nsome problems in the current study and discuss future directions. Source codes\nare available at <a class=\"link-external link-https\" href=\"https://github.com/trainingl/STG4Traffic\" rel=\"external noopener nofollow\">this https URL</a>."},
{"title": "UnLoc: A Universal Localization Method for Autonomous Vehicles using  LiDAR, Radar and/or Camera Input", "authors": ["Muhammad Ibrahim", "Naveed Akhtar", "Saeed Anwar", "Ajmal Mian"], "url": "https://arxiv.org/abs/2307.00741", "abstract": "Localization is a fundamental task in robotics for autonomous navigation.\nExisting localization methods rely on a single input data modality or train\nseveral computational models to process different modalities. This leads to\nstringent computational requirements and sub-optimal results that fail to\ncapitalize on the complementary information in other data streams. This paper\nproposes UnLoc, a novel unified neural modeling approach for localization with\nmulti-sensor input in all weather conditions. Our multi-stream network can\nhandle LiDAR, Camera and RADAR inputs for localization on demand, i.e., it can\nwork with one or more input sensors, making it robust to sensor failure. UnLoc\nuses 3D sparse convolutions and cylindrical partitioning of the space to\nprocess LiDAR frames and implements ResNet blocks with a slot attention-based\nfeature filtering module for the Radar and image modalities. We introduce a\nunique learnable modality encoding scheme to distinguish between the input\nsensor data. Our method is extensively evaluated on Oxford Radar RobotCar,\nApolloSouthBay and Perth-WA datasets. The results ascertain the efficacy of our\ntechnique."},
{"title": "ImDiffusion: Imputed Diffusion Models for Multivariate Time Series  Anomaly Detection", "authors": ["Yuhang Chen", "Chaoyun Zhang", "Minghua Ma", "Yudong Liu", "Ruomeng Ding", "Bowen Li", "Shilin He", "Saravan Rajmohan", "Qingwei Lin", "Dongmei Zhang"], "url": "https://arxiv.org/abs/2307.00754", "abstract": "Anomaly detection in multivariate time series data is of paramount importance\nfor ensuring the efficient operation of large-scale systems across diverse\ndomains. However, accurately detecting anomalies in such data poses significant\nchallenges. Existing approaches, including forecasting and reconstruction-based\nmethods, struggle to address these challenges effectively. To overcome these\nlimitations, we propose a novel anomaly detection framework named ImDiffusion,\nwhich combines time series imputation and diffusion models to achieve accurate\nand robust anomaly detection. The imputation-based approach employed by\nImDiffusion leverages the information from neighboring values in the time\nseries, enabling precise modeling of temporal and inter-correlated\ndependencies, reducing uncertainty in the data, thereby enhancing the\nrobustness of the anomaly detection process. ImDiffusion further leverages\ndiffusion models as time series imputers to accurately capturing complex\ndependencies. We leverage the step-by-step denoised outputs generated during\nthe inference process to serve as valuable signals for anomaly prediction,\nresulting in improved accuracy and robustness of the detection process.\n<br>We evaluate the performance of ImDiffusion via extensive experiments on\nbenchmark datasets. The results demonstrate that our proposed framework\nsignificantly outperforms state-of-the-art approaches in terms of detection\naccuracy and timeliness. ImDiffusion is further integrated into the real\nproduction system in Microsoft and observe a remarkable 11.4% increase in\ndetection F1 score compared to the legacy approach. To the best of our\nknowledge, ImDiffusion represents a pioneering approach that combines\nimputation-based techniques with time series anomaly detection, while\nintroducing the novel use of diffusion models to the field."},
{"title": "Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence  Time-Series Forecasting", "authors": ["Nhat Thanh Tran", "Jack Xin"], "url": "https://arxiv.org/abs/2307.00493", "abstract": "We study a fast local-global window-based attention method to accelerate\nInformer for long sequence time-series forecasting. While window attention is\nlocal and a considerable computational saving, it lacks the ability to capture\nglobal token information which is compensated by a subsequent Fourier transform\nblock. Our method, named FWin, does not rely on query sparsity hypothesis and\nan empirical approximation underlying the ProbSparse attention of Informer.\nThrough experiments on univariate and multivariate datasets, we show that FWin\ntransformers improve the overall prediction accuracies of Informer while\naccelerating its inference speeds by 40 to 50 %. We also show in a nonlinear\nregression model that a learned FWin type attention approaches or even\noutperforms softmax full attention based on key vectors extracted from an\nInformer model's full attention layer acting on time series data."},
{"title": "PatternGPT :A Pattern-Driven Framework for Large Language Model Text  Generation", "authors": ["Le Xiao", "Xin Shan"], "url": "https://arxiv.org/abs/2307.00470", "abstract": "Large language models(LLMs) have shown excellent text generation\ncapabilities, but there is still much space for improvement in accuracy,\nsometimes with grammatical errors, semantic inaccuracies, and contextual\nincoherence, which seriously affect the reliability of the models. These\nproblems may originate from the difficulties and limitations encountered in the\npattern extraction stage of large language models. How to utilize the\ngenerative power of large language models to generate as many possible patterns\nthat help solve problems and find the optimal patterns from them, so as to use\npatterns to guide large language models to generate good content, has become a\ncurrent research hotspot. In this paper, we propose a pattern extraction and\nselection framework, PatternGPT, which generates rich patterns through the\nextraction ability of large language models and draws on the idea of federation\nlearning, where multiple agents collaborate with each other to generate diverse\npatterns. High-quality patterns are selected by defining criteria and\noptimization algorithms to personalize the guidance of the model generation\nprocess. PatternGPT has the advantages of generating diverse and useful\npatterns, extending relevant knowledge, facilitating efficient pattern use and\ntransfer, and optimizing the quality of generated results and user experience,\nwhich provides an effective method for optimizing the text generation\ncapability of large language models and is expected to drive further\ndevelopment in the field of intelligent dialogue and content generation. It is\nexpected to promote further development in the field of intelligent dialogue\nand content generation."},
{"title": "Human-to-Human Interaction Detection", "authors": ["Zhenhua Wang", "Kaining Ying", "Jiajun Meng", "Jifeng Ning", "Cong Bai"], "url": "https://arxiv.org/abs/2307.00464", "abstract": "A comprehensive understanding of interested human-to-human interactions in\nvideo streams, such as queuing, handshaking, fighting and chasing, is of\nimmense importance to the surveillance of public security in regions like\ncampuses, squares and parks. Different from conventional human interaction\nrecognition, which uses choreographed videos as inputs, neglects concurrent\ninteractive groups, and performs detection and recognition in separate stages,\nwe introduce a new task named human-to-human interaction detection (HID). HID\ndevotes to detecting subjects, recognizing person-wise actions, and grouping\npeople according to their interactive relations, in one model. First, based on\nthe popular AVA dataset created for action detection, we establish a new HID\nbenchmark, termed AVA-Interaction (AVA-I), by adding annotations on interactive\nrelations in a frame-by-frame manner. AVA-I consists of 85,254 frames and\n86,338 interactive groups, and each image includes up to 4 concurrent\ninteractive groups. Second, we present a novel baseline approach SaMFormer for\nHID, containing a visual feature extractor, a split stage which leverages a\nTransformer-based model to decode action instances and interactive groups, and\na merging stage which reconstructs the relationship between instances and\ngroups. All SaMFormer components are jointly trained in an end-to-end manner.\nExtensive experiments on AVA-I validate the superiority of SaMFormer over\nrepresentative methods. The dataset and code will be made public to encourage\nmore follow-up studies."},
{"title": "Cloud Ensemble Learning for Fault Diagnosis of Rolling Bearings with  Stochastic Configuration Networks", "authors": ["Wei Dai", "Jiang Liu", "Lanhao Wang"], "url": "https://arxiv.org/abs/2307.00507", "abstract": "Fault diagnosis of rolling bearings is of great significance for\npost-maintenance in rotating machinery, but it is a challenging work to\ndiagnose faults efficiently with a few samples. Additionally, faults commonly\noccur with randomness and fuzziness due to the complexity of the external\nenvironment and the structure of rolling bearings, hindering effective mining\nof fault characteristics and eventually restricting accuracy of fault\ndiagnosis. To overcome these problems, stochastic configuration network (SCN)\nbased cloud ensemble learning, called SCN-CEL, is developed in this work.\nConcretely, a cloud feature extraction method is first developed by using a\nbackward cloud generator of normal cloud model to mine the uncertainty of fault\ninformation. Then, a cloud sampling method, which generates enough cloud\ndroplets using bidirectional cloud generator, is proposed to extend the cloud\nfeature samples. Finally, an ensemble model with SCNs is developed to\ncomprehensively characterize the uncertainty of fault information and advance\nthe generalization performance of fault diagnosis machine. Experimental results\ndemonstrate that the proposed method indeed performs favorably for\ndistinguishing fault categories of rolling bearings in the few shot scenarios."},
{"title": "Efficient Subclass Segmentation in Medical Images", "authors": ["Linrui Dai", "Wenhui Lei", "Xiaofan Zhang"], "url": "https://arxiv.org/abs/2307.00257", "abstract": "As research interests in medical image analysis become increasingly\nfine-grained, the cost for extensive annotation also rises. One feasible way to\nreduce the cost is to annotate with coarse-grained superclass labels while\nusing limited fine-grained annotations as a complement. In this way,\nfine-grained data learning is assisted by ample coarse annotations. Recent\nstudies in classification tasks have adopted this method to achieve\nsatisfactory results. However, there is a lack of research on efficient\nlearning of fine-grained subclasses in semantic segmentation tasks. In this\npaper, we propose a novel approach that leverages the hierarchical structure of\ncategories to design network architecture. Meanwhile, a task-driven data\ngeneration method is presented to make it easier for the network to recognize\ndifferent subclass categories. Specifically, we introduce a Prior Concatenation\nmodule that enhances confidence in subclass segmentation by concatenating\npredicted logits from the superclass classifier, a Separate Normalization\nmodule that stretches the intra-class distance within the same superclass to\nfacilitate subclass segmentation, and a HierarchicalMix model that generates\nhigh-quality pseudo labels for unlabeled samples by fusing only similar\nsuperclass regions from labeled and unlabeled images. Our experiments on the\nBraTS2021 and ACDC datasets demonstrate that our approach achieves comparable\naccuracy to a model trained with full subclass annotations, with limited\nsubclass annotations and sufficient superclass annotations. Our approach offers\na promising solution for efficient fine-grained subclass segmentation in\nmedical images. Our code is publicly available here."},
{"title": "An ML approach to resolution of singularities", "authors": ["Gergely B\u00e9rczi", "Honglu Fan", "Mingcong Zeng"], "url": "https://arxiv.org/abs/2307.00252", "abstract": "The solution set of a system of polynomial equations typically contains\nill-behaved, singular points. Resolution is a fundamental process in geometry\nin which we replace singular points with smooth points, while keeping the rest\nof the solution set unchanged. Resolutions are not unique: the usual way to\ndescribe them involves repeatedly performing a fundamental operation known as\n\"blowing-up\", and the complexity of the resolution highly depends on certain\nchoices. The process can be translated into various versions of a 2-player\ngame, the so-called Hironaka game, and a winning strategy for the first player\nprovides a solution to the resolution problem. In this paper we introduce a new\napproach to the Hironaka game that uses reinforcement learning agents to find\noptimal resolutions of singularities. In certain domains, the trained model\noutperforms state-of-the-art selection heuristics in total number of polynomial\nadditions performed, which provides a proof-of-concept that recent developments\nin machine learning have the potential to improve performance of algorithms in\nsymbolic computation."},
{"title": "Conformer LLMs -- Convolution Augmented Large Language Models", "authors": ["Prateek Verma"], "url": "https://arxiv.org/abs/2307.00461", "abstract": "This work builds together two popular blocks of neural architecture, namely\nconvolutional layers and Transformers, for large language models (LLMs).\nNon-causal conformers are used ubiquitously in automatic speech recognition.\nThis work aims to adapt these architectures in a causal setup for training\nLLMs. Transformers decoders effectively capture long-range dependencies over\nseveral modalities and form a core backbone of modern advancements in machine\nlearning. Convolutional architectures have been popular in extracting features\nin domains such as raw 1-D signals, speech, and images, to name a few. In this\npaper, by combining local and global dependencies over latent representations\nusing causal convolutional filters and Transformer, we achieve significant\ngains in performance. This work showcases a robust speech architecture that can\nbe integrated and adapted in a causal setup beyond speech applications for\nlarge-scale language modeling."}
]